\chapter{\sffamily Generalised MAP inference}

{\bfseries\sffamily Concept.} To largely generalise the procedure of statistical inference for any model using an algorithm which builds from techniques we developed in the previous chapter. When we say `statistical inference' here; we specifically mean computing the maximum a posteriori (MAP) estimate for any arbitrary stochastic model which has been defined in the stochadex simulator. In order for our algorithm to evaluate the MAP of a model, we show that the user must specify the prior distribution over model parameters, and the model must itself be defined within the stochadex. In this chapter, we will discuss some concepts which are commonplace within the field of Bayesian inference and provide a few simple examples of how our algorithm might work in various instances. For the mathematically-inclined, this chapter will give a very brief exposition for Bayesian statistical inference metholodology --- in particular, how it relates to the evaluation of MAP estimates. For the programmers, the software described in this chapter lives in the public Git repository: \href{https://github.com/umbralcalc/learnadex}{https://github.com/umbralcalc/learnadex}.


\section{\sffamily Inference formalism}

In Bayesian inference, one applies Bayes' rule to the problem of statistically inferring a model from some dataset. This typically involves the following formula for a posterior distribution
%%
\begin{align}
{\cal P}_{{\sf t}+1}(z \vert Y) \propto {\cal L}_{{\sf t}+1}(Y\vert z){\cal P} (z) \label{eq:bayes-rule} \,.
\end{align}
%%
In the formula above, one relates the prior probability distribution over a parameter set ${\cal P} (z)$ and the likelihood ${\cal L}_{{\sf t}+1}(Y\vert z)$  of some data matrix $Y$ up to timestep ${\sf t}+1$ given the parameters $z$ of a model to the posterior probability distribution of parameters given the data ${\cal P}_{{\sf t}+1}(z \vert Y)$ up to some proportionality constant. All this may sound a bit technical in statistical language, so it can also be helpful to summarise what the formula above states verbally as follows: the initial (prior) state of knowledge about the parameters $z$ we want to learn can be updated by some likelihood function of the data to give a new state of knowledge about the values for $z$ (the `posterior' probability). 

From the point of view of statistical inference, if we seek to maximise ${\cal P}_{{\sf t}+1}(z \vert Y)$ --- or its logarithm --- in Eq.~(\ref{eq:bayes-rule}) with respect to $z$, we will obtain what is known as a maximum posteriori (MAP) estimate of the parameters. In fact, we have already encountered this metholodology in the previous chapter when discussing the algorithm which obtains the best fit parameters for the empirical probability filter. In this case; while it appears that we optimised the log-likelihood directly as our objective function, one can easily show that this is also technically equivalent obtaining a MAP estimate where one chooses a specfic prior ${\cal P} (z) \propto 1$ (typically known as a `flat prior').

How might we calulate the posterior in practice with some arbitrary stochastic process model that has been defined in the stochadex? In order to make the comparison to a real dataset, any stochadex model of interest will always need to be able to generate observations which can be directly compared to the data. To formalise this a little; a stochadex model could be represented as a map from $z$ to a set of stochastic outputs ${\sf Y}_{{\sf t}+1}(z), {\sf Y}_{{\sf t}}(z), \dots$ that are directly comparable to the rows in the real data matrix $Y$. The values in $Y$ may only represent a noisy or partial measurement of the latent state of the simulation $x$, so a more complete picture can be provided by the following probabilistic relation
%%
\begin{align}
P_{{\sf t}+1}({\sf y} \vert z) = \int_{\omega_{{\sf t}+1}}{\rm d}x\, P_{{\sf t}+1}({\sf y} \vert x)P_{{\sf t}+1}(x\vert z) \label{eq:simulation-measurement} \,,
\end{align}
%%
where, in practical terms, the measurement probability $P_{{\sf t}+1}({\sf y} \vert x)$ of ${\sf y}={\sf Y}_{{\sf t}+1}$ given $x=X_{{\sf t}+1}$ can be represented by sampling from another stochastic process which takes the state of the stochadex simulation as input. Given that we have this capability to compare like-for-like between the data and the simulation; the next problem is to figure out how this comparison between two sequences of vectors can be done in a way which ensures the the statistics of the posterior are ultimately respected. 

For an arbitrary simulation model which is defined by the stochadex, the likelihood in Eq.~(\ref{eq:bayes-rule}) is typically not describable as a simple function or distribution. While we could train the probability filter we derived in the previous chapter to match the simulation; to do this well would require having an exact formula for the conditional probability, and this is not always easy to derive in the general case. Instead, there is a class of Bayesian inference methods which we shall lean on to help us compute the posterior distribution (and hence the MAP), which are known as `Likelihood-Free' methods~\cite{sisson2018handbook,price2018bayesian,wood2010statistical,drovandi2022comparison}.

`Likelihood-Free' methods work by separating out the components of the posterior which relate to the closeness of rows in ${\sf Y}$ (the matrix which comprises ${\sf Y}_{{\sf t}+1}(z), {\sf Y}_{{\sf t}}(z), \dots$) to the rows in $Y$ from the components which relate the state $x$ and parameters $z$ of the simulation stochastically to ${\sf Y}$. To achieve this separation, we can make use of chaining conditional probability like this
%%
\begin{align}
{\cal P}_{{\sf t}+1}(x,z\vert Y)=\prod_{{\sf t}'=0}^{{\sf t}+1} \bigg[ \int_{\varpi_{{\sf t}'}} {\rm d}{\sf y}' \, {\cal P}_{{\sf t}'}({\sf y}'\vert Y) \bigg] P_{{\sf t}+1}(x,z \vert {\sf Y}) \label{eq:likelihhood-free-posterior} \,,
\end{align}
%%
where $\varpi_{{\sf t}'}$ here corresponds to the domain of ${\sf y}'$ at time ${\sf t}'$.

As we demonstrated in the previous chapter, it's possible for us to also optimise a probability distribution ${\cal P}_{{\sf t}+1}({\sf y}\vert Y) = P_{{\sf t}+1}[\,{\sf y};M_{{\sf t}+1}(Y),C_{{\sf t}+1}(Y), \dots ]$ for each step in time to match the statistics of the measurements in $Y$ as well as possible, given some statistics $M_{{\sf t}+1}(Y)$ and $C_{{\sf t}+1}(Y)$. We do not necessarily need to obtain these statistics from the probability filter estimation method, but could instead try to fit them via some other objective function. Either way, this represents a lossy \emph{compression} of the data we want to fit the simulation to, and so the best possible fit is desirable; regardless of overfitting. This choice to summarise the data with statistics means we are using what is known as a Bayesian Synthetic Likelihood (BSL) method~\cite{price2018bayesian,wood2010statistical} instead of another class of methods which approximate an objective function directly using a proximity kernel --- known as Approximate Bayesian Computation (ABC) methods~\cite{sisson2018handbook}.

Let's consider a few concrete examples of $P_{{\sf t}+1}[\,{\sf y};M_{{\sf t}+1}(Y),C_{{\sf t}+1}(Y), \dots ]$. If the data measurements were well-described by a multivariate normal distribution, then one would use terms like
%%
\begin{align}
P_{{\sf t}+1}[\,{\sf y};M_{{\sf t}+1}(Y),C_{{\sf t}+1}(Y), \dots ] = {\sf MultivariateNormalPDF}[\,{\sf y};M_{{\sf t}+1}(Y),C_{{\sf t}+1}(Y)]\,,
\end{align}
%%
where $M_{{\sf t}+1}(Y)$ and $C_{{\sf t}+1}(Y)$ would be estimated from the data measurements in $Y$. Similarly, if the data measurements were instead better described by a Poisson distribution, we might disregard the need for a covariance matrix statistic $C_{{\sf t}+1}(Y)$ and instead use
%%
\begin{align}
P_{{\sf t}+1}[\,{\sf y};M_{{\sf t}+1}(Y),C_{{\sf t}+1}(Y), \dots ] = {\sf PoissonPMF}[\,{\sf y};M_{{\sf t}+1}(Y)]\,.
\end{align}
%%
Once again, here, we would need to determine $M_{{\sf t}+1}(Y)$ from $Y$. The more statistically-inclined readers may notice that the probability mass function here would require the integrals in Eq.~(\ref{eq:likelihhood-free-posterior}) to be replaced with summations over the relevant domains.

Eq.~(\ref{eq:likelihhood-free-posterior}) demonstrates how one can construct a statistically meaningful way to compare the sequence of real data measurements $Y_{{\sf t}+1}, Y_{{\sf t}}, \dots$ to their modelled equivalents ${\sf Y}_{{\sf t}+1}(z), {\sf Y}_{{\sf t}}(z), \dots$. But we still haven't shown how to compute $P_{{\sf t}+1}(x,z\vert {\sf Y})$ for a given simulation, and this can be the most challenging part. To begin with, we can reapply Bayes' rule and the chaining of conditional probability to find 
%%
\begin{align}
P_{{\sf t}+1}(x,z\vert {\sf Y}) \propto P_{{\sf t}+1}({\sf y}\vert x)P_{{\sf t}+1}(x\vert z)P_{{\sf t}}(z\vert {\sf Y}') \,,
\end{align}
%%
where, to keep the expression simpler, we are leveraging our notation in previous chapters to use ${\sf Y}'$ as indicating all of the past rows of simulation measurements up to ${\sf Y}_{{\sf t}}$. 

The relationship between $P_{{\sf t}+1}({\sf y}\vert x)$ and previous timesteps can be directly inferred from the probabilistic iteration formula that we introduced in the previous chapter. So we can map probabilities of $x$ throughout time and learned information about the state of the system can be applied from previous values, given $z$. But is there a similar relationship we might consider for $P_{{\sf t}+1}(z\vert {\sf Y})$? Yes there is! The marginalisation
%%
\begin{align} 
P_{{\sf t}+1}(z\vert {\sf Y}) = \int_{\omega_{{\sf t}+1}} {\rm d}x \, P_{{\sf t}+1}(x,z\vert {\sf Y}) &\propto \bigg[ \int_{\omega_{{\sf t}+1}} {\rm d}x \, P_{{\sf t}+1}({\sf y}\vert x) P_{{\sf t}+1}(x\vert z) \bigg] P_{{\sf t}}(z\vert {\sf Y}') \label{eq:z-update}\,,
\end{align}
%%
shows how the $z$ updates can occur in an iterative fashion. The reader may also recognize the factor above in brackets as Eq.~(\ref{eq:simulation-measurement}). If there is a fixed window size in the past all the way back to ${\sf t}'={\sf s}$ beyond which we do not consider the real data relevant to inform the current state, then Eq.~(\ref{eq:z-update}) implies that
%%
\begin{align}
P_{{\sf t}+1}(z\vert {\sf Y}) &\propto \prod_{{\sf t}'={\sf s}}^{{\sf t}}\bigg[ \int_{\omega_{{\sf t}'+1}} {\rm d}x \, P_{{\sf t}'+1}({\sf y}\vert x) P_{{\sf t}'+1}(x\vert z) \bigg] P_{{\sf s}}(z) \label{eq:windowed-z-update}\,.
\end{align}
%%

\section{\sffamily Online/offline inference algorithms}

\textcolor{red}{Got up to here...
\begin{itemize}
\item{describe both online and offline learning routes in this chapter so introduce the concept of $n$ for offline and then do away with it for online and replace it with an ensemble and $z\rightarrow Z_{{\sf t}}$ --- describe how the two have many similarities from a probabilistic perspective though}
\item{for online learning, the $P_{{\sf t}+1}(z\vert {\sf Y}) \leftarrow P_{{\sf t}}(z\vert {\sf Y}')$ update can be handles by applying importance-sampled wieghting to the previous distribution samples and then either MC resampling or optimising to get the MAP from the resulting filtered statistics distribution}
\item{the online algorithm is specifically: 1. sample new values for $z$ from the current $z$ distribution estimate, keeping only the current MAP sample and replacing all of the other samples with new proposal values 2. run the iterations again from the window size back in time ${\sf t}'={\sf s}$ all the way up to ${\sf t}'={\sf t}+1$ 3. determine the new MAP sample through Eq.~(\ref{eq:windowed-z-update}) and Eq.~(\ref{eq:likelihhood-free-posterior}) 4. iterate time forward, stream new datapoints in and recompute the likelihood statistics to go into Eq.~(\ref{eq:likelihhood-free-posterior}) all the way up to the next $z$ distribution refresh checkpoint 
5. recompute the estimated mean and variance of the samples using the importance weightings to get a new estimated $z$ distribution and go to 1. }
\item{offline algorithm sets the prior to the inference block in Eq.~(\ref{eq:windowed-z-update}) with 
$$P^{n+1}_{{\sf t}+1}(z) = \frac{1}{n}\sum_{n'=0}^n\int_{\upsilon_{n'}} \,{\rm d}z' \, P^{(n+1)n'}(z\vert z')\, P^{n'}_{{\sf t}+1}(z')$$ 
whereas the online algorithm sets with prior with 
$$P_{{\sf t}+1}(z) = \frac{1}{{\sf t}}\sum_{{\sf t}'=0}^{{\sf t}}\int_{\upsilon_{{\sf t}'}} \,{\rm d}z' \, P_{({\sf t}+1){\sf t}'}(z\vert z')\, P_{{\sf t}'}(z')$$}
\end{itemize}}

We now have an objective function to optimise values of $z$ with respect to in Eq.~(\ref{eq:likelihhood-free-posterior}) but, in practice, this optimisation problem typically has several layers of difficulty to it. Since the model has been defined by its stochastically generated samples ${\sf Y}_{{\sf t}+1}(z), {\sf Y}_{{\sf t}}(z), \dots$, the objective function will manifestly be stochastic too. Another layer of difficulty is that gradients of the objective function are not immediately computable and so navigation around the optimisation domain could be difficult, especially in high-dimensional problems. To solve this issue in a way which maintains the generality of our approach, it turns out that we can rely on another application of our probability filter in the next section.

Let's now consider a process which represents the progress made by an optimiser towards the optimum value, whose state is defined by the parameters $z$. If we assume that all the elements of $z$ are continuous\footnote{We can modify this expression to work for discrete variables too.} in their domain $\upsilon$, one representation of this process we might consider is
%%
\begin{align} 
P^{n+1}_{{\sf t}+1}(z) &= \frac{1}{n}\sum_{n'=0}^n\int_{\upsilon_{n'}} \,{\rm d}z' \, P^{(n+1)n'}(z\vert z')\, P^{n'}_{{\sf t}+1}(z'\vert {\sf Y}_{{\sf t}+1}, Y_{{\sf t}+1}) \label{eq:posterior-process} \,,
\end{align}
%%
where $n$ is the iteration number of the optimisation, and hence ${\cal P}^{n+1}_{{\sf t}+1}$ corresponds to a different candidate posterior distribution to one at any of the previous steps ${\cal P}^{n'}_{{\sf t}+1}$. Eq.~(\ref{eq:posterior-process}) can represent the convergence of a probabilistic optimisation algorithm --- represented in particular by $P^{(n+1)n'}(z\vert z')$ --- towards the true posterior distribution of $z$. As such an algorithm converges, we can recompute (and hence iteratively improve) the MAP estimate with respect to each iteration of the posterior.

Readers with some machine learning experience may be familiar with the classic exploration vs exploitation tradeoffs. It's clear that these tradeoffs will manifest in our case here when trying to strike a balance between iterating the posterior distribution in Eq.~(\ref{eq:posterior-process}) and optimizing the current posterior with respect to $z$ to compute the MAP. 

Readers may also have recognized that Eq.~(\ref{eq:posterior-process}) has the same structure as the generalised probabilistic description we introduced previously. This structure enables us to reuse all of the exposition we provided for the probabilistic filter and highlights how the filter itself can be used in the algorithm to optimise the posterior.   

If we now synthesize both of these observations together, we can see how a stochastic variant of the well-known Expectation-Maximisation Algorithm~\cite{hartley1958maximum, dempster1977maximum, murphy2012machine} naturally emerges.


