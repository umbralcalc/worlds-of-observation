<!DOCTYPE html>
<html lang="" xml:lang="" xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta charset="utf-8"/>
  <meta content="pandoc" name="generator"/>
  <meta content="width=device-width, initial-scale=1.0, user-scalable=yes" name="viewport"/>
  <meta content="Robert J. Hardwick" name="author"/>
  <style>
   code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6" type="text/javascript">
  </script>
  <script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <link href="style.css" rel="stylesheet"/>
 </head>
 <body class="w3-animate-opacity">
  <!-- Navbar -->
  <div class="w3-top">
   <div class="w3-bar w3-light-grey w3-card w3-left-align w3-large">
    <a class="w3-bar-item w3-button w3-hide-medium w3-hide-large w3-right w3-padding-large w3-hover-white w3-large w3-light-grey" href="javascript:void(0);" onclick="myFunction()" title="Toggle Navigation Menu">
     <i class="fa fa-bars">
     </i>
    </a>
    <a class="w3-bar-item w3-button w3-padding-large w3-white" href="#">
     <font color="grey">
      Home
     </font>
    </a>
    <a class="w3-bar-item w3-button w3-hide-small w3-padding-large w3-hover-white" href="https://github.com/umbralcalc/diffusing-ideas/raw/main/book.pdf">
     <font color="grey">
      Download The PDF
     </font>
    </a>
   </div>
   <!-- Navbar on small screens -->
   <div class="w3-bar-block w3-white w3-hide w3-hide-large w3-hide-medium w3-large" id="navDemo">
    <a class="w3-bar-item w3-button w3-padding-large" href="#">
     Download The PDF
    </a>
   </div>
  </div>
  <hr/>
  <hr/>
  <hr/>
  <hr/>
  <header class="w3-container w3-animate-opacity w3-light-grey w3-center" style="padding:20px 16px">
   <h1 class="title">
    Diffusing Ideas
   </h1>
   <h3 class="subtitle">
    Software, noise and building mathematical toys
   </h3>
  </header>
  <p class="author">
  <hr/>
  <hr/>
  <p>
   <br/>
   <span>
    <strong>
     Robert J. Hardwick
     <br/>
    </strong>
   </span>
   Shared by the author under an
   <a href="https://opensource.org/licenses/MIT">
    MIT License
   </a>
   .
   <br/>
   The code to compile this book is open source and can be found in this repository:
   <a href="https://github.com/umbralcalc/diffusing-ideas">
    https://github.com/umbralcalc/diffusing-ideas
   </a>
   .
  </p>
  <h2 class="unnumbered" id="introduction">
   Introduction
  </h2>
  <p>
   <em>
    Diffusing Ideas
   </em>
   is a book of research exploration and software development which I have written for the interest of mathematically-inclined programmers and computational scientists. It’s the output of many interrelated projects over several years which have sought to generalise the computational mathematics of simulating, statistically inferring, manipulating and automatically controlling stochastic phenomena as far as possible.
  </p>
  <p>
   The book accompanies a lot of new open-source scientific software written predominantly in Go
   <span class="citation" data-cites="golang">
    [1]
   </span>
   , and with some Python
   <span class="citation" data-cites="pythonlang">
    [2]
   </span>
   to top it off. A major motivation for creating these new tools is to prepare a foundation of code from which to develop new and more complex applications. I also hope that the resulting framework will enable anyone to explore and study new phenomena effectively, regardless of their scientific background.
  </p>
  <p>
   The need to properly test all this software has also provided a wonderful excuse to study and play with an extensive range of mathematical toy models. I’ve chosen these models based on a fairly broad background of interests, but also to illustrate the remarkable cross-disciplinary applicability of stochastic processes. However, I’ve often found that mathematical formalities can obscure the computations that a programmer must implement. So, while I’ve tried to be as ambitious as possible with the level of technical sophistication in these models, I’ve also tried to write the expressions in a computer-friendly way where feasible
   <a class="footnote-ref" href="#fn1" id="fnref1" role="doc-noteref">
    <sup>
     1
    </sup>
   </a>
   and have added code snippets throughout.
  </p>
  <p>
   A quick note on the code: any software that I describe in this book (including the software which compiles the book itself
   <span class="citation" data-cites="diffusingideasbookgithub">
    [3]
   </span>
   ) will always be shared under a MIT License
   <span class="citation" data-cites="mitlicense">
    [4]
   </span>
   in a public Git repository.
   <a class="footnote-ref" href="#fn2" id="fnref2" role="doc-noteref">
    <sup>
     2
    </sup>
   </a>
   Forking these repositories and submitting pull requests for new features or applications is strongly encouraged too, though I apologise in advance if I don’t follow these up very quickly as all of this work has to be conducted independently in free time, outside of work hours.
  </p>
  <p>
   No quest would be complete without a guide, so I think this introduction should end with a list of the key milestones in the book; comprising its four major parts. These parts each correspond to answering one of the following interdependent research questions:
  </p>
  <ol>
   <li>
    <p>
     <span>
      How do we simulate a general set of stochastic phenomena?
     </span>
    </p>
   </li>
   <li>
    <p>
     <span>
      How do we then learn/identify the answer to
      <span>
       <strong>
        Part 1
       </strong>
      </span>
      from real-world data?
     </span>
    </p>
   </li>
   <li>
    <p>
     <span>
      How do we simulate a general set of control policies to interact with the answer to
      <span>
       <strong>
        Part 1
       </strong>
      </span>
      ?
     </span>
    </p>
   </li>
   <li>
    <p>
     <span>
      How do we then optimise the answer to
      <span>
       <strong>
        Part 3
       </strong>
      </span>
      to achieve a specified control objective?
     </span>
    </p>
   </li>
  </ol>
  <p>
  </p>
  <h1 id="part-1.-how-do-we-simulate-a-general-set-of-stochastic-phenomena">
   Part 1.
   <span>
    How do we simulate a general set of stochastic phenomena?
   </span>
  </h1>
  <h2 id="building-a-generalised-simulator">
   Building a generalised simulator
  </h2>
  <p>
   <span>
    <strong>
     Concept.
    </strong>
   </span>
   To design and build a generalised simulation engine that is able to generate samples from a ‘Pokédex’ of possible stochastic processes that a researcher might encounter. A ‘Pokédex’ here is just my fanciful description for a very general class of multidimensional stochastic processes that pop up everywhere in taming the mathematical wilds of real-world phenomena, and which also leads to a name for the software itself: the ‘stochadex’. With such a thing pre-built and self-contained, it can become the basis upon which to build generalised software solutions for a lot of different problems. For the mathematically-inclined, this chapter will require the introduction of a new formalism which we shall refer back to throughout the book. For the programmers, the public Git repository for the code that is described in this chapter can be found here:
   <a href="https://github.com/umbralcalc/stochadex">
    https://github.com/umbralcalc/stochadex
   </a>
   .
  </p>
  <h3 id="computational-formalism">
   Computational formalism
  </h3>
  <p>
   Before we dive into software design of the stochadex, we need to mathematically define the general computational approach that we’re going to take. Because the language of stochastic processes is primarily mathematics, I’d argue this step is essential in enabling a really general description. From experience, it seems reasonable to start by writing down the following formula which describes iterating some arbitrary process forward in time (by one finite step) and adding a new row each to some matrix
   <span class="math inline">
    \(X' \rightarrow X\)
   </span>
   \[\begin{aligned}
X^{i}_{{\sf t}+1} &amp;= F^{i}_{{\sf t}+1}(X',{\sf t}) \,, \label{eq:x-step-def}\end{aligned}\tag{1}\]
   where:
   <span class="math inline">
    \(i\)
   </span>
   is an index for the dimensions of the ‘state’ space;
   <span class="math inline">
    \({\sf t}\)
   </span>
   is the current time index for either a discrete-time process or some discrete approximation to a continuous-time process;
   <span class="math inline">
    \(X\)
   </span>
   is the next version of
   <span class="math inline">
    \(X'\)
   </span>
   after one timestep (and hence one new row has been added); and
   <span class="math inline">
    \(F^i_{{\sf t}+1}(X',{\sf t})\)
   </span>
   as the latest element of an arbitrary matrix-valued function. As we shall discuss shortly,
   <span class="math inline">
    \(F^i_{{\sf t}+1}(X',{\sf t})\)
   </span>
   may represent not just operations on deterministic variables, but also on stochastic ones. There is also no requirement for the function to be continuous.
  </p>
  <figure>
   <img alt="" id="fig:fundamental-loop" src="images/fundamental-loop.drawio.png" style="width:10cm"/>
   <figcaption>
    Graph representation of Eq. (
    <a data-reference="1" data-reference-type="ref" href="#1">
     1
    </a>
    ).
   </figcaption>
  </figure>
  <p>
   The basic computational idea here is illustrated in Fig.
   <a data-reference="fig:fundamental-loop" data-reference-type="ref" href="#fig:fundamental-loop">
    1.1
   </a>
   ; we iterate the matrix
   <span class="math inline">
    \(X\)
   </span>
   forward in time by a row, and use its previous version
   <span class="math inline">
    \(X'\)
   </span>
   as an entire matrix input into a function which populates the elements of its latest rows. In Go you could easily write something with the same idea in it, and the code would probably look like this.
  </p>
  <div class="sourceCode" data-language="Go" id="cb1">
   <pre class="sourceCode go"><code class="sourceCode go"><span id="cb1-1"><a aria-hidden="true" href="#cb1-1"></a><span class="kw">type</span> StateVector  []<span class="dt">float64</span></span>
<span id="cb1-2"><a aria-hidden="true" href="#cb1-2"></a><span class="kw">type</span> StateHistory []StateVector</span>
<span id="cb1-3"><a aria-hidden="true" href="#cb1-3"></a></span>
<span id="cb1-4"><a aria-hidden="true" href="#cb1-4"></a><span class="co">// iterate the state history forward in time by one step</span></span>
<span id="cb1-5"><a aria-hidden="true" href="#cb1-5"></a><span class="kw">func</span> IterationFormula(</span>
<span id="cb1-6"><a aria-hidden="true" href="#cb1-6"></a>    stateHistory StateHistory, </span>
<span id="cb1-7"><a aria-hidden="true" href="#cb1-7"></a>    timeStepNumber <span class="dt">int</span>,</span>
<span id="cb1-8"><a aria-hidden="true" href="#cb1-8"></a>) StateVector {</span>
<span id="cb1-9"><a aria-hidden="true" href="#cb1-9"></a>    <span class="kw">for</span> t, stateVector := <span class="kw">range</span> stateHistory {</span>
<span id="cb1-10"><a aria-hidden="true" href="#cb1-10"></a>        <span class="kw">for</span> i, stateElement := <span class="kw">range</span> stateVector {</span>
<span id="cb1-11"><a aria-hidden="true" href="#cb1-11"></a>            <span class="co">// do something</span></span>
<span id="cb1-12"><a aria-hidden="true" href="#cb1-12"></a>        }</span>
<span id="cb1-13"><a aria-hidden="true" href="#cb1-13"></a>    }</span>
<span id="cb1-14"><a aria-hidden="true" href="#cb1-14"></a>    <span class="kw">return</span> newestStateVector</span>
<span id="cb1-15"><a aria-hidden="true" href="#cb1-15"></a>}</span></code></pre>
  </div>
  <p>
   Pretty simple! But why go to all this trouble of storing matrix inputs for previous values of the same process? It’s true that this is mostly redundant for
   <em>
    Markovian
   </em>
   phenomena, i.e., processes where their only memory of their history is the most recent value they took. However, for a large class of stochastic processes a full memory
   <a class="footnote-ref" href="#fn3" id="fnref3" role="doc-noteref">
    <sup>
     3
    </sup>
   </a>
   of past values is essential to consistently construct the sample paths moving forward. This is true in particular for
   <em>
    non-Markovian
   </em>
   phenomena, where the latest values don’t just depend on the immediately previous ones but can depend on values which occured much earlier in the process as well.
  </p>
  <p>
   For more complex physical models and integrators, the distinct notions of ‘numerical timestep’ and ‘total elapsed continuous time’ will crop up quite frequently. Hence, before moving on further details, it will be important to define the total elapsed time variable
   <span class="math inline">
    \(t({\sf t})\)
   </span>
   for processes which are defined in continuous time. Assuming that we have already defined some function
   <span class="math inline">
    \(\delta t({\sf t})\)
   </span>
   which returns the specific change in continuous time that corresponds to the step
   <span class="math inline">
    \({\sf t}-1 \rightarrow {\sf t}\)
   </span>
   , we will always be able to compute the total elapsed time through the relation
   \[\begin{aligned}
t({\sf t}) &amp;= \sum^{{\sf t}}_{{\sf t}'=0}\delta t({\sf t}') \label{eq:t-steps-sum} \,.\end{aligned}\tag{2}\]
   This seems a lot of effort, no? Well it’s important to remember that our steps in continuous time may not be constant, so by defining the
   <span class="math inline">
    \(\delta t({\sf t})\)
   </span>
   function and summing over it we can enable this flexibility in the computation. In case the summation notation is no fun for programmers; in Go we’re implicitly doing this.
  </p>
  <div class="sourceCode" data-language="Go" id="cb2">
   <pre class="sourceCode go"><code class="sourceCode go"><span id="cb2-1"><a aria-hidden="true" href="#cb2-1"></a><span class="co">// get the next increment from this step number forward in</span></span>
<span id="cb2-2"><a aria-hidden="true" href="#cb2-2"></a><span class="co">// some unit of time, e.g., seconds</span></span>
<span id="cb2-3"><a aria-hidden="true" href="#cb2-3"></a><span class="kw">func</span> TimeIncrementFunction(timeStepNumber <span class="dt">int</span>) <span class="dt">float64</span> {</span>
<span id="cb2-4"><a aria-hidden="true" href="#cb2-4"></a>    <span class="co">// compute the next increment</span></span>
<span id="cb2-5"><a aria-hidden="true" href="#cb2-5"></a>    <span class="kw">return</span> nextTimeIncrement</span>
<span id="cb2-6"><a aria-hidden="true" href="#cb2-6"></a>}</span>
<span id="cb2-7"><a aria-hidden="true" href="#cb2-7"></a></span>
<span id="cb2-8"><a aria-hidden="true" href="#cb2-8"></a><span class="co">// compute the total time elapsed up to the input step number in</span></span>
<span id="cb2-9"><a aria-hidden="true" href="#cb2-9"></a><span class="co">// some unit of time, e.g., seconds</span></span>
<span id="cb2-10"><a aria-hidden="true" href="#cb2-10"></a><span class="kw">func</span> ElapsedTimeFunction(timeStepNumber <span class="dt">int</span>) <span class="dt">float64</span> {</span>
<span id="cb2-11"><a aria-hidden="true" href="#cb2-11"></a>    totalElapsedTime := <span class="dv">0</span><span class="fl">.0</span></span>
<span id="cb2-12"><a aria-hidden="true" href="#cb2-12"></a>    <span class="kw">for</span> t := <span class="dv">0</span>; t &lt; timeStepNumber; t++ {</span>
<span id="cb2-13"><a aria-hidden="true" href="#cb2-13"></a>        totalElapsedTime += TimeIncrementFunction(t)</span>
<span id="cb2-14"><a aria-hidden="true" href="#cb2-14"></a>    }</span>
<span id="cb2-15"><a aria-hidden="true" href="#cb2-15"></a>    <span class="kw">return</span> totalElapsedTime</span>
<span id="cb2-16"><a aria-hidden="true" href="#cb2-16"></a>}</span></code></pre>
  </div>
  <p>
   So, now that we’ve mathematically defined a really general notion of iterating the stochastic process forward in time, it makes sense to discuss some simple examples. For instance, it is frequently possible to split
   <span class="math inline">
    \(F\)
   </span>
   up into deteministic (denoted
   <span class="math inline">
    \(D\)
   </span>
   ) and stochastic (denoted
   <span class="math inline">
    \(S\)
   </span>
   ) matrix-valued functions like so
   \[\begin{aligned}
&amp; F^{i}_{{\sf t}+1}(X',{\sf t}) = D^{i}_{{\sf t}+1}(X',{\sf t}) + S^{i}_{{\sf t}+1}(X',{\sf t}) \,.\end{aligned}\tag{3}\]
   In the case of stochastic processes with continuous sample paths, it’s also nearly always the case with mathematical models of real-world systems that the deterministic part will at least contain the term
   <span class="math inline">
    \(D^{i}_{{\sf t}+1}(X',{\sf t}) = X^i_{\sf t}\)
   </span>
   because the overall system is described by some stochastic differential equation. This is not a really requirement in our general formalism, however.
  </p>
  <p>
   What about the stochastic term? For example, if we wanted to consider a
   <em>
    Wiener process noise
   </em>
   , we can define
   <span class="math inline">
    \(W^i_{{\sf t}}\)
   </span>
   is a sample from a Wiener process for each of the state dimensions indexed by
   <span class="math inline">
    \(i\)
   </span>
   and our formalism becomes
   \[\begin{aligned}
&amp; S^{i}_{{\sf t}+1}(X',{\sf t}) = W^i_{{\sf t}+1}-W^i_{\sf t} \label{eq:wiener}\,.\end{aligned}\tag{4}\]
   One draws the increments
   <span class="math inline">
    \(W^i_{{\sf t}+1}-W^i_{\sf t}\)
   </span>
   from a normal distribution with a mean of
   <span class="math inline">
    \(0\)
   </span>
   and a variance equal to the length of continuous time that the step corresponded to
   <span class="math inline">
    \(\delta t({\sf t}+1)\)
   </span>
   , i.e., the probability density
   <span class="math inline">
    \(P^i_{{\sf t}+1}(x)\)
   </span>
   of the increments
   <span class="math inline">
    \(x^i=W^i_{{\sf t}+1}-W^i_{\sf t}\)
   </span>
   is
   \[\begin{aligned}
P^i_{{\sf t}+1}(x) &amp;= {\sf NormalPDF}[x^i;0,\delta t({\sf t}+1)] \,.\end{aligned}\tag{5}\]
   Note that for state spaces with dimensions
   <span class="math inline">
    \(&gt;1\)
   </span>
   , we could also allow for non-trivial cross-correlations between the noises in each dimension. In Go, the Wiener process can be represented within our formalism like this.
  </p>
  <div class="sourceCode" data-language="Go" id="cb3">
   <pre class="sourceCode go"><code class="sourceCode go"><span id="cb3-1"><a aria-hidden="true" href="#cb3-1"></a><span class="kw">import</span> (</span>
<span id="cb3-2"><a aria-hidden="true" href="#cb3-2"></a>    <span class="st">"math"</span></span>
<span id="cb3-3"><a aria-hidden="true" href="#cb3-3"></a>    <span class="st">"math/rand"</span></span>
<span id="cb3-4"><a aria-hidden="true" href="#cb3-4"></a>)</span>
<span id="cb3-5"><a aria-hidden="true" href="#cb3-5"></a></span>
<span id="cb3-6"><a aria-hidden="true" href="#cb3-6"></a><span class="co">// generate a new Wiener process increment for a state element</span></span>
<span id="cb3-7"><a aria-hidden="true" href="#cb3-7"></a><span class="kw">func</span> NewWienerProcessIncrement(timeStepNumber <span class="dt">int</span>) <span class="dt">float64</span> {</span>
<span id="cb3-8"><a aria-hidden="true" href="#cb3-8"></a>    <span class="co">// use the time increment function we defined earlier</span></span>
<span id="cb3-9"><a aria-hidden="true" href="#cb3-9"></a>    timeIncrement := TimeIncrementFunction(timeStepNumber)</span>
<span id="cb3-10"><a aria-hidden="true" href="#cb3-10"></a>    <span class="co">// multiply by the square-root here as </span></span>
<span id="cb3-11"><a aria-hidden="true" href="#cb3-11"></a>    <span class="co">// it is proportional to the standard deviation</span></span>
<span id="cb3-12"><a aria-hidden="true" href="#cb3-12"></a>    value := math.Sqrt(timeIncrement) * rand.NormFloat64()</span>
<span id="cb3-13"><a aria-hidden="true" href="#cb3-13"></a>    <span class="kw">return</span> value</span>
<span id="cb3-14"><a aria-hidden="true" href="#cb3-14"></a>}</span>
<span id="cb3-15"><a aria-hidden="true" href="#cb3-15"></a></span>
<span id="cb3-16"><a aria-hidden="true" href="#cb3-16"></a><span class="co">// returns the state vector from the S(X',t) function we defined </span></span>
<span id="cb3-17"><a aria-hidden="true" href="#cb3-17"></a><span class="co">// for the Wiener Process in the main text above</span></span>
<span id="cb3-18"><a aria-hidden="true" href="#cb3-18"></a><span class="kw">func</span> SFunctionWienerProcess(</span>
<span id="cb3-19"><a aria-hidden="true" href="#cb3-19"></a>    stateHistory StateHistory, </span>
<span id="cb3-20"><a aria-hidden="true" href="#cb3-20"></a>    timeStepNumber <span class="dt">int</span>,</span>
<span id="cb3-21"><a aria-hidden="true" href="#cb3-21"></a>) StateVector {</span>
<span id="cb3-22"><a aria-hidden="true" href="#cb3-22"></a>    <span class="co">// we don't care about the state history so the noise = Markovian</span></span>
<span id="cb3-23"><a aria-hidden="true" href="#cb3-23"></a>    sFunctionValue := <span class="bu">make</span>(StateVector, <span class="dv">0</span>)</span>
<span id="cb3-24"><a aria-hidden="true" href="#cb3-24"></a>    <span class="kw">for</span> _ = <span class="kw">range</span> stateVector {</span>
<span id="cb3-25"><a aria-hidden="true" href="#cb3-25"></a>        increment := NewWienerProcessIncrement(timeStepNumber)</span>
<span id="cb3-26"><a aria-hidden="true" href="#cb3-26"></a>        sFunctionValue = <span class="bu">append</span>(sFunctionValue, increment)</span>
<span id="cb3-27"><a aria-hidden="true" href="#cb3-27"></a>    }</span>
<span id="cb3-28"><a aria-hidden="true" href="#cb3-28"></a>    <span class="kw">return</span> sFunctionValue</span>
<span id="cb3-29"><a aria-hidden="true" href="#cb3-29"></a>}</span></code></pre>
  </div>
  <p>
   In another example, to model
   <em>
    geometric Brownian motion noise
   </em>
   we would simply have to multiply
   <span class="math inline">
    \(X^i_{\sf t}\)
   </span>
   to the Wiener process like so
   \[\begin{aligned}
&amp; S^{i}_{{\sf t}+1}(X',{\sf t}) = X^i_{\sf t}(W^i_{{\sf t}+1}-W^i_{\sf t})\label{eq:gbm} \,.\end{aligned}\tag{6}\]
   Here we have implicitly adopted the Itô interpretation to describe this stochastic integration. Given a carefully-defined integration scheme other interpretations of the noise would also be possible with our formalism too, e.g., Stratonovich
   <a class="footnote-ref" href="#fn4" id="fnref4" role="doc-noteref">
    <sup>
     4
    </sup>
   </a>
   or others within the more general ‘
   <span class="math inline">
    \(\alpha\)
   </span>
   -family’
   <span class="citation" data-cites="van1992stochastic risken1996fokker rog-will-2000">
    [5]–[7]
   </span>
   . The Go code for any of these should hoepfully be fairly straightforward to deduce based on the lines I’ve already written above.
  </p>
  <p>
   We can imagine even more general processes that are still Markovian. One example of these in a single-dimension state space would be to define the noise through some general function of the Wiener process like so
   \[\begin{aligned}
S^0_{{\sf t}+1}(X',{\sf t}) &amp;= g[W^0_{{\sf t}+1},t({\sf t}+1)]-g[W^0_{\sf t}, t({\sf t})] \\
&amp;= \bigg[ \frac{\partial g}{\partial t} + \frac{1}{2}\frac{\partial^2 g}{\partial x^2} \bigg] \delta t ({\sf t}+1) + \frac{\partial g}{\partial x} (W^0_{{\sf t}+1}-W^0_{\sf t}) \label{eq:general-wiener}\,,\end{aligned}\tag{7}\]
   where
   <span class="math inline">
    \(g(x,t)\)
   </span>
   is some continuous function of its arguments which has been expanded out with Itô’s Lemma on the second line. Note also that the computations in Eq. (
   <a data-reference="7" data-reference-type="ref" href="#7">
    7
   </a>
   ) could be performed with numerical derivatives in principle, even if the function were extremely complicated. This is unlikely to be the best way to describe the process of interest, however, the mathematical expressions above can still be made a bit more meaningful to the programmer in this way. The code would probably look something like this.
  </p>
  <div class="sourceCode" data-language="Go" id="cb4">
   <pre class="sourceCode go"><code class="sourceCode go"><span id="cb4-1"><a aria-hidden="true" href="#cb4-1"></a><span class="co">// some function</span></span>
<span id="cb4-2"><a aria-hidden="true" href="#cb4-2"></a><span class="kw">func</span> G(wienerProcessSample <span class="dt">float64</span>, timeStepNumber <span class="dt">int</span>) <span class="dt">float64</span> {</span>
<span id="cb4-3"><a aria-hidden="true" href="#cb4-3"></a>    <span class="co">// return something</span></span>
<span id="cb4-4"><a aria-hidden="true" href="#cb4-4"></a>}</span>
<span id="cb4-5"><a aria-hidden="true" href="#cb4-5"></a></span>
<span id="cb4-6"><a aria-hidden="true" href="#cb4-6"></a><span class="co">// discretely represents the dg/dt expression in the equation above</span></span>
<span id="cb4-7"><a aria-hidden="true" href="#cb4-7"></a><span class="kw">func</span> DgDt(</span>
<span id="cb4-8"><a aria-hidden="true" href="#cb4-8"></a>    newWienerProcessSample <span class="dt">float64</span>, </span>
<span id="cb4-9"><a aria-hidden="true" href="#cb4-9"></a>    previousWienerProcessSample <span class="dt">float64</span>,</span>
<span id="cb4-10"><a aria-hidden="true" href="#cb4-10"></a>    timeStepNumber <span class="dt">int</span>,</span>
<span id="cb4-11"><a aria-hidden="true" href="#cb4-11"></a>) <span class="dt">float64</span> {</span>
<span id="cb4-12"><a aria-hidden="true" href="#cb4-12"></a>    <span class="kw">return</span> (G(newWienerProcessSample, timeStepNumber) - </span>
<span id="cb4-13"><a aria-hidden="true" href="#cb4-13"></a>        G(previousWienerProcessSample, timeStepNumber)) / </span>
<span id="cb4-14"><a aria-hidden="true" href="#cb4-14"></a>        TimeIncrementFunction(timeStepNumber)</span>
<span id="cb4-15"><a aria-hidden="true" href="#cb4-15"></a>}</span>
<span id="cb4-16"><a aria-hidden="true" href="#cb4-16"></a></span>
<span id="cb4-17"><a aria-hidden="true" href="#cb4-17"></a><span class="co">// discretely represents the dg/dx expression in the equation above</span></span>
<span id="cb4-18"><a aria-hidden="true" href="#cb4-18"></a><span class="kw">func</span> DgDx(</span>
<span id="cb4-19"><a aria-hidden="true" href="#cb4-19"></a>    newWienerProcessSample <span class="dt">float64</span>, </span>
<span id="cb4-20"><a aria-hidden="true" href="#cb4-20"></a>    previousWienerProcessSample <span class="dt">float64</span>,</span>
<span id="cb4-21"><a aria-hidden="true" href="#cb4-21"></a>    timeStepNumber <span class="dt">int</span>,</span>
<span id="cb4-22"><a aria-hidden="true" href="#cb4-22"></a>) <span class="dt">float64</span> {</span>
<span id="cb4-23"><a aria-hidden="true" href="#cb4-23"></a>    <span class="kw">return</span> (G(newWienerProcessSample, timeStepNumber) - </span>
<span id="cb4-24"><a aria-hidden="true" href="#cb4-24"></a>        G(previousWienerProcessSample, timeStepNumber)) / </span>
<span id="cb4-25"><a aria-hidden="true" href="#cb4-25"></a>        (newWienerProcessSample - previousWienerProcessSample)</span>
<span id="cb4-26"><a aria-hidden="true" href="#cb4-26"></a>}</span>
<span id="cb4-27"><a aria-hidden="true" href="#cb4-27"></a></span>
<span id="cb4-28"><a aria-hidden="true" href="#cb4-28"></a><span class="co">// discretely represents the d^2g/dx^2 expression in the equation above</span></span>
<span id="cb4-29"><a aria-hidden="true" href="#cb4-29"></a><span class="kw">func</span> D2gDx2(</span>
<span id="cb4-30"><a aria-hidden="true" href="#cb4-30"></a>    <span class="co">// newDgDx and previousDgDx could be passed in here</span></span>
<span id="cb4-31"><a aria-hidden="true" href="#cb4-31"></a>    newWienerProcessSample <span class="dt">float64</span>, </span>
<span id="cb4-32"><a aria-hidden="true" href="#cb4-32"></a>    previousWienerProcessSample <span class="dt">float64</span>,</span>
<span id="cb4-33"><a aria-hidden="true" href="#cb4-33"></a>    timeStepNumber <span class="dt">int</span>,</span>
<span id="cb4-34"><a aria-hidden="true" href="#cb4-34"></a>) <span class="dt">float64</span> {</span>
<span id="cb4-35"><a aria-hidden="true" href="#cb4-35"></a>    <span class="co">// newDgDx and previousDgDx are the result of applying the function </span></span>
<span id="cb4-36"><a aria-hidden="true" href="#cb4-36"></a>    <span class="co">// for dg/dx defined above on two different timesteps</span></span>
<span id="cb4-37"><a aria-hidden="true" href="#cb4-37"></a>    <span class="kw">return</span> (newDgDx - previousDgDx) / </span>
<span id="cb4-38"><a aria-hidden="true" href="#cb4-38"></a>        (newWienerProcessSample - previousWienerProcessSample)</span>
<span id="cb4-39"><a aria-hidden="true" href="#cb4-39"></a>}</span></code></pre>
  </div>
  <p>
   Let’s now look at a more complicated type of noise. For example,
   <em>
    fractional Brownian motion
   </em>
   <span class="math inline">
    \([B_{H}]_{\sf t}\)
   </span>
   with Hurst exponent
   <span class="math inline">
    \(H\)
   </span>
   . Following Ref.
   <span class="citation" data-cites="decreusefond1999stochastic">
    [8]
   </span>
   , we can simulate this process in one of our state space dimensions by modifying a standard Wiener process like so
   \[\begin{aligned}
S^{0}_{{\sf t}+1}(X',{\sf t}) &amp;= \frac{(W^0_{{\sf t}+1} - W^0_{\sf t})}{\delta t({\sf t})}\int^{t({\sf t}+1)}_{t({\sf t})}{\rm d}t' \frac{(t'-t)^{H-\frac{1}{2}}}{\Gamma (H+\frac{1}{2})} {}_2F_1 \bigg( H-\frac{1}{2};\frac{1}{2}-H;H+\frac{1}{2};1-\frac{t'}{t}\bigg) \label{eq:fbm} \,,\end{aligned}\tag{8}\]
   where
   <span class="math inline">
    \(S^{0}_{{\sf t}+1}(X',{\sf t})=[B_{H}]_{{\sf t}+1}-[B_{H}]_{{\sf t}}\)
   </span>
   . The integral in Eq. (
   <a data-reference="8" data-reference-type="ref" href="#8">
    8
   </a>
   ) can be approximated using an appropriate numerical procedure (like the trapezium rule). In the expression above
   <span class="math inline">
    \({}_2F_1\)
   </span>
   and
   <span class="math inline">
    \(\Gamma\)
   </span>
   are the ordinary hypergeometric and gamma functions, respectively. A discretised form of this integral is written below in Go to try and disentangle some of the complexity in the expression.
  </p>
  <div class="sourceCode" data-language="Go" id="cb5">
   <pre class="sourceCode go"><code class="sourceCode go"><span id="cb5-1"><a aria-hidden="true" href="#cb5-1"></a><span class="kw">import</span> <span class="st">"scientificgo.org/special"</span></span>
<span id="cb5-2"><a aria-hidden="true" href="#cb5-2"></a></span>
<span id="cb5-3"><a aria-hidden="true" href="#cb5-3"></a><span class="co">// computes the integral term in the fractional Brownian motion process</span></span>
<span id="cb5-4"><a aria-hidden="true" href="#cb5-4"></a><span class="co">// defined above using a scientific library</span></span>
<span id="cb5-5"><a aria-hidden="true" href="#cb5-5"></a><span class="kw">func</span> FractionalBrownianMotionIntegral(</span>
<span id="cb5-6"><a aria-hidden="true" href="#cb5-6"></a>    currentTime <span class="dt">float64</span>,</span>
<span id="cb5-7"><a aria-hidden="true" href="#cb5-7"></a>    nextTime <span class="dt">float64</span>,</span>
<span id="cb5-8"><a aria-hidden="true" href="#cb5-8"></a>    hurstExponent <span class="dt">float64</span>,</span>
<span id="cb5-9"><a aria-hidden="true" href="#cb5-9"></a>    numberOfIntegrationSteps <span class="dt">int</span>,</span>
<span id="cb5-10"><a aria-hidden="true" href="#cb5-10"></a>) <span class="dt">float64</span> {</span>
<span id="cb5-11"><a aria-hidden="true" href="#cb5-11"></a>    integralStepSize := (nextTime - currentTime)/<span class="dt">float64</span>(numberOfIntegrationSteps)</span>
<span id="cb5-12"><a aria-hidden="true" href="#cb5-12"></a>    a := []<span class="dt">float64</span>{hurstExponent - <span class="dv">0</span><span class="fl">.5</span>, <span class="dv">0</span><span class="fl">.5</span> - hurstExponent}</span>
<span id="cb5-13"><a aria-hidden="true" href="#cb5-13"></a>    b := []<span class="dt">float64</span>{hurstExponent + <span class="dv">0</span><span class="fl">.5</span>}</span>
<span id="cb5-14"><a aria-hidden="true" href="#cb5-14"></a>    integralValue := <span class="dv">0</span><span class="fl">.0</span></span>
<span id="cb5-15"><a aria-hidden="true" href="#cb5-15"></a>    <span class="co">// implements the trapezium rule in a loop over the steps</span></span>
<span id="cb5-16"><a aria-hidden="true" href="#cb5-16"></a>    <span class="co">// between the current and the next point in time</span></span>
<span id="cb5-17"><a aria-hidden="true" href="#cb5-17"></a>    <span class="kw">for</span> t := <span class="dv">0</span>; t &lt; numberOfIntegrationSteps; t++ {</span>
<span id="cb5-18"><a aria-hidden="true" href="#cb5-18"></a>        t1 := currentTime + <span class="dt">float64</span>(t)*integralStepSize</span>
<span id="cb5-19"><a aria-hidden="true" href="#cb5-19"></a>        t2 := t1 + integralStepSize</span>
<span id="cb5-20"><a aria-hidden="true" href="#cb5-20"></a>        functionValue1 := (math.Pow(t1-currentTime, hurstExponent<span class="dv">-0</span><span class="fl">.5</span>) /</span>
<span id="cb5-21"><a aria-hidden="true" href="#cb5-21"></a>        math.Gamma(hurstExponent<span class="fl">+0.5</span>)) *</span>
<span id="cb5-22"><a aria-hidden="true" href="#cb5-22"></a>        special.HypPFQ(a, b, <span class="dv">1</span><span class="fl">.0</span>-t1/currentTime)</span>
<span id="cb5-23"><a aria-hidden="true" href="#cb5-23"></a>        functionValue2 := (math.Pow(t2-currentTime, hurstExponent<span class="dv">-0</span><span class="fl">.5</span>) /</span>
<span id="cb5-24"><a aria-hidden="true" href="#cb5-24"></a>        math.Gamma(hurstExponent<span class="fl">+0.5</span>)) *</span>
<span id="cb5-25"><a aria-hidden="true" href="#cb5-25"></a>        special.HypPFQ(a, b, <span class="dv">1</span><span class="fl">.0</span>-t2/currentTime)</span>
<span id="cb5-26"><a aria-hidden="true" href="#cb5-26"></a>        integralValue += <span class="dv">0</span><span class="fl">.5</span> * (functionValue1 + functionValue2) * integralStepSize</span>
<span id="cb5-27"><a aria-hidden="true" href="#cb5-27"></a>    }</span>
<span id="cb5-28"><a aria-hidden="true" href="#cb5-28"></a>    <span class="kw">return</span> integralValue</span>
<span id="cb5-29"><a aria-hidden="true" href="#cb5-29"></a>}</span></code></pre>
  </div>
  <p>
   So far we have mostly been discussing noises with continuous sample paths, but we can easily adapt our computation to discontinuous sample paths as well. For instance,
   <em>
    Poisson process noises
   </em>
   would generally take the form
   \[\begin{aligned}
S^{i}_{{\sf t}+1}(X',{\sf t}) &amp;= [N_{\lambda}]^i_{{\sf t}+1}-[N_{\lambda}]^i_{\sf t}\,,\end{aligned}\tag{9}\]
   where
   <span class="math inline">
    \([N_{\lambda}]^i_{\sf t}\)
   </span>
   is a sample from a Poisson process with rate
   <span class="math inline">
    \(\lambda\)
   </span>
   . One can think of this process as counting the number of events which have occured up to the given interval of time, where the intervals between each succesive event are exponentially distributed with mean
   <span class="math inline">
    \(1/\lambda\)
   </span>
   . Such a simple counting process could be simulated exactly by explicitly setting a newly-drawn exponential variate to the next continuous time jump
   <span class="math inline">
    \({\delta t}({\sf t}+1)\)
   </span>
   and iterating the counter. Other exact methods exist to handle more complicated processes involving more than one type of ‘event’, such as the Gillespie algorithm
   <span class="citation" data-cites="gillespie1977exact">
    [9]
   </span>
   — though these techniques are not always be applicable in every situation.
  </p>
  <p>
   Is using step size variation always possible? If we consider a
   <em>
    time-inhomogeneous Poisson process noise
   </em>
   , which would generally take the form
   \[\begin{aligned}
S^{i}_{{\sf t}+1}(X',{\sf t}) &amp;= [N_{\lambda ({\sf t}+1)}]^i_{{\sf t}+1}-[N_{\lambda ({\sf t})}]^i_{\sf t}\,,\end{aligned}\tag{10}\]
   the rate
   <span class="math inline">
    \(\lambda ({\sf t})\)
   </span>
   has become a deterministically-varying function in time. In this instance, it likely not be accurate to simulate this process by drawing exponential intervals with a mean of
   <span class="math inline">
    \(1/\lambda ({\sf t})\)
   </span>
   because this mean could have changed by the end of the interval which was drawn. An alternative approach (which is more generally capable of simulating jump processes but is an approximation) first uses a small time interval
   <span class="math inline">
    \(\tau\)
   </span>
   such that the most likely thing to happen in this period is nothing, and then the probability of the event occuring is simply given by
   \[\begin{aligned}
p({\sf event}) &amp;= \frac{\lambda ({\sf t})}{\lambda ({\sf t}) + \frac{1}{\tau}} \label{eq:rejection}\,.\end{aligned}\tag{11}\]
   This idea can be applied to phenomena with an arbitrary number of events and works well as a generalised approach to event-based simulation, though its main limitation is worth remembering; in order to make the approximation good,
   <span class="math inline">
    \(\tau\)
   </span>
   often must be quite small and hence our simulator must churn through a lot of steps. From now on we’ll refer to this well-known technique as the
   <em>
    rejection method
   </em>
   . The following chunk of Go code may also help to understand this concept from the programmer’s perspective.
  </p>
  <div class="sourceCode" data-language="Go" id="cb6">
   <pre class="sourceCode go"><code class="sourceCode go"><span id="cb6-1"><a aria-hidden="true" href="#cb6-1"></a><span class="co">// generate new event rates as a function of timestep</span></span>
<span id="cb6-2"><a aria-hidden="true" href="#cb6-2"></a><span class="kw">func</span> EventRateLambdaFunction(timeStepNumber <span class="dt">int</span>) <span class="dt">float64</span> {</span>
<span id="cb6-3"><a aria-hidden="true" href="#cb6-3"></a>    <span class="co">// return a new rate</span></span>
<span id="cb6-4"><a aria-hidden="true" href="#cb6-4"></a>}</span>
<span id="cb6-5"><a aria-hidden="true" href="#cb6-5"></a></span>
<span id="cb6-6"><a aria-hidden="true" href="#cb6-6"></a><span class="co">// get the next exponentially-distributed time increment</span></span>
<span id="cb6-7"><a aria-hidden="true" href="#cb6-7"></a><span class="kw">func</span> ExpDistributedTimeIncrementFunction(</span>
<span id="cb6-8"><a aria-hidden="true" href="#cb6-8"></a>    smallTimeInterval <span class="dt">float64</span>,</span>
<span id="cb6-9"><a aria-hidden="true" href="#cb6-9"></a>) <span class="dt">float64</span> {</span>
<span id="cb6-10"><a aria-hidden="true" href="#cb6-10"></a>    nextTimeIncrement := smallTimeInterval * rand.ExpFloat64()</span>
<span id="cb6-11"><a aria-hidden="true" href="#cb6-11"></a>    <span class="kw">return</span> nextTimeIncrement</span>
<span id="cb6-12"><a aria-hidden="true" href="#cb6-12"></a>}</span>
<span id="cb6-13"><a aria-hidden="true" href="#cb6-13"></a></span>
<span id="cb6-14"><a aria-hidden="true" href="#cb6-14"></a><span class="co">// returns the time-inhomogeneous Poisson process S(X',t) term </span></span>
<span id="cb6-15"><a aria-hidden="true" href="#cb6-15"></a><span class="kw">func</span> SFunctionInhomogeneousPoissonProcess(</span>
<span id="cb6-16"><a aria-hidden="true" href="#cb6-16"></a>    stateHistory StateHistory,</span>
<span id="cb6-17"><a aria-hidden="true" href="#cb6-17"></a>    timeStepNumber <span class="dt">int</span>,</span>
<span id="cb6-18"><a aria-hidden="true" href="#cb6-18"></a>) StateVector {</span>
<span id="cb6-19"><a aria-hidden="true" href="#cb6-19"></a>    <span class="co">// notice how the noise is also Markovian here too</span></span>
<span id="cb6-20"><a aria-hidden="true" href="#cb6-20"></a>    <span class="kw">var</span> smallTimeInterval <span class="dt">float64</span></span>
<span id="cb6-21"><a aria-hidden="true" href="#cb6-21"></a>    sFunctionValue := <span class="bu">make</span>(StateVector, <span class="dv">0</span>)</span>
<span id="cb6-22"><a aria-hidden="true" href="#cb6-22"></a>    <span class="kw">for</span> element = <span class="kw">range</span> stateVector {</span>
<span id="cb6-23"><a aria-hidden="true" href="#cb6-23"></a>        timeIncrement := ExpDistributedTimeIncrementFunction(</span>
<span id="cb6-24"><a aria-hidden="true" href="#cb6-24"></a>            smallTimeInterval,</span>
<span id="cb6-25"><a aria-hidden="true" href="#cb6-25"></a>        )</span>
<span id="cb6-26"><a aria-hidden="true" href="#cb6-26"></a>        <span class="co">// specify an arbitrary function of time for the event rate here</span></span>
<span id="cb6-27"><a aria-hidden="true" href="#cb6-27"></a>        eventRateLambda := EventRateLambdaFunction(timeStepNumber)</span>
<span id="cb6-28"><a aria-hidden="true" href="#cb6-28"></a>        prob := eventRateLambda /</span>
<span id="cb6-29"><a aria-hidden="true" href="#cb6-29"></a>            (eventRateLambda + <span class="dv">1</span><span class="fl">.0</span>/timeIncrement)</span>
<span id="cb6-30"><a aria-hidden="true" href="#cb6-30"></a>        <span class="co">// note that the difference between steps for a Poisson</span></span>
<span id="cb6-31"><a aria-hidden="true" href="#cb6-31"></a>        <span class="co">// process can only ever be 0 or 1</span></span>
<span id="cb6-32"><a aria-hidden="true" href="#cb6-32"></a>        element := <span class="dv">0</span><span class="fl">.0</span></span>
<span id="cb6-33"><a aria-hidden="true" href="#cb6-33"></a>        <span class="kw">if</span> rand.Float64() &lt; prob {</span>
<span id="cb6-34"><a aria-hidden="true" href="#cb6-34"></a>            element = <span class="dv">1</span><span class="fl">.0</span></span>
<span id="cb6-35"><a aria-hidden="true" href="#cb6-35"></a>        }</span>
<span id="cb6-36"><a aria-hidden="true" href="#cb6-36"></a>        sFunctionValue = <span class="bu">append</span>(sFunctionValue, element)</span>
<span id="cb6-37"><a aria-hidden="true" href="#cb6-37"></a>    }</span>
<span id="cb6-38"><a aria-hidden="true" href="#cb6-38"></a>    <span class="kw">return</span> sFunctionValue</span>
<span id="cb6-39"><a aria-hidden="true" href="#cb6-39"></a>}</span></code></pre>
  </div>
  <p>
   There are a few extensions to the simple Poisson process that introduce additional stochastic processes.
   <em>
    Cox (doubly-stochastic) processes
   </em>
   , for instance, are basically where we replace the time-dependent rate
   <span class="math inline">
    \(\lambda ({\sf t})\)
   </span>
   with independent samples from some other stochastic process
   <span class="math inline">
    \(\Lambda ({\sf t})\)
   </span>
   . For example, a Neyman-Scott process
   <span class="citation" data-cites="neyman1958statistical">
    [10]
   </span>
   can be mapped as a special case of this because it uses a Poisson process on top of another Poisson process to create maps of spatially-distributed points. In our formalism, a two-state implementation of the Cox process noise would look like
   \[\begin{aligned}
S^{0}_{{\sf t}+1}(X',{\sf t}) &amp;= \Lambda ({\sf t}+1) \\
S^{1}_{{\sf t}+1}(X',{\sf t}) &amp;= [N_{S^{0}_{{\sf t}+1}}]^i_{{\sf t}+1}-[N_{S^{0}_{{\sf t}}}]^i_{\sf t}\,.\end{aligned}\tag{12}\]
   This process could be simulated using the Go code we wrote for the time-inhomogeneous Poisson process above — where we would just replace
   <code>
    EventRateLambdaFunction
   </code>
   with a function that generates the stochastic rate
   <span class="math inline">
    \(\Lambda ({\sf t})\)
   </span>
   .
  </p>
  <p>
   Another extension is
   <em>
    compound Poisson process noise
   </em>
   , where it’s the count values
   <span class="math inline">
    \([N_{\lambda}]^i_{\sf t}\)
   </span>
   which are replaced by independent samples
   <span class="math inline">
    \([J_{\lambda}]^i_{\sf t}\)
   </span>
   from another probability distribution, i.e.,
   \[\begin{aligned}
S^{i}_{{\sf t}+1}(X',{\sf t}) &amp;= [J_{\lambda}]^i_{{\sf t}+1}-[J_{\lambda}]^i_{\sf t}\,.\end{aligned}\tag{13}\]
   Note that the rejection method of Eq. (
   <a data-reference="11" data-reference-type="ref" href="#11">
    11
   </a>
   ) can be employed effectively to simulate any of these extensions as long as a sufficiently small
   <span class="math inline">
    \(\tau\)
   </span>
   is chosen. Once again, the Go code we wrote above would be sufficient to simulate this process with one tweak: replace the allocation of the
   <code>
    element
   </code>
   variable to
   <code>
    element = 1.0
   </code>
   with the output of a function which generates the
   <span class="math inline">
    \([J_{\lambda}]^i_{\sf t}\)
   </span>
   samples.
  </p>
  <p>
   All of the examples we have discussed so far are Markovian. Given that we have explicitly constructed the formalism to handle non-Markovian phenomena as well, it would be worthwhile going some examples of this kind of process too.
   <em>
    Self-exciting process noises
   </em>
   would generally take the form
   \[\begin{aligned}
S^{0}_{{\sf t}+1}(X',{\sf t}) &amp;= {\cal I}_{{\sf t}+1} (X',{\sf t}) \\
S^{1}_{{\sf t}+1}(X',{\sf t}) &amp;= [N_{S^{0}_{{\sf t}+1}}]^i_{{\sf t}+1}-[N_{S^{0}_{{\sf t}}}]^i_{\sf t} \,,\end{aligned}\tag{14}\]
   where the stochastic rate
   <span class="math inline">
    \({\cal I}_{{\sf t}+1} (X',{\sf t})\)
   </span>
   now depends on the history explicitly. Amongst other potential inputs we can see, e.g., Hawkes processes
   <span class="citation" data-cites="hawkes1971spectra">
    [11]
   </span>
   as an example of above by substituting
   \[\begin{aligned}
{\cal I}_{{\sf t}+1} (X',{\sf t}) &amp;= \mu + \sum^{{\sf t}}_{{\sf t}'=0}\gamma [t({\sf t})-t({\sf t}')](S^{1}_{{\sf t}'}-S^{1}_{{\sf t}'-1}) \,,\end{aligned}\tag{15}\]
   where
   <span class="math inline">
    \(\gamma\)
   </span>
   is the ‘exciting kernel’ and
   <span class="math inline">
    \(\mu\)
   </span>
   is some constant background rate. In order to simulate a Hawkes process using our formalism, the Go code would look like this.
  </p>
  <div class="sourceCode" data-language="Go" id="cb7">
   <pre class="sourceCode go"><code class="sourceCode go"><span id="cb7-1"><a aria-hidden="true" href="#cb7-1"></a><span class="kw">func</span> ExcitingKernel(</span>
<span id="cb7-2"><a aria-hidden="true" href="#cb7-2"></a>    currentTime <span class="dt">float64</span>, somePreviousTime <span class="dt">float64</span>,</span>
<span id="cb7-3"><a aria-hidden="true" href="#cb7-3"></a>) <span class="dt">float64</span> {</span>
<span id="cb7-4"><a aria-hidden="true" href="#cb7-4"></a>    <span class="co">// returns some number</span></span>
<span id="cb7-5"><a aria-hidden="true" href="#cb7-5"></a>}</span>
<span id="cb7-6"><a aria-hidden="true" href="#cb7-6"></a></span>
<span id="cb7-7"><a aria-hidden="true" href="#cb7-7"></a><span class="co">// returns the Hawkes process S(X',t) term</span></span>
<span id="cb7-8"><a aria-hidden="true" href="#cb7-8"></a><span class="kw">func</span> SFunctionHawkesProcess(</span>
<span id="cb7-9"><a aria-hidden="true" href="#cb7-9"></a>    stateHistory StateHistory,</span>
<span id="cb7-10"><a aria-hidden="true" href="#cb7-10"></a>    timeStepNumber <span class="dt">int</span>,</span>
<span id="cb7-11"><a aria-hidden="true" href="#cb7-11"></a>) StateVector {</span>
<span id="cb7-12"><a aria-hidden="true" href="#cb7-12"></a>    <span class="co">// notice how the noise is also Markovian here too</span></span>
<span id="cb7-13"><a aria-hidden="true" href="#cb7-13"></a>    <span class="kw">var</span> smallTimeInterval <span class="dt">float64</span></span>
<span id="cb7-14"><a aria-hidden="true" href="#cb7-14"></a>    sFunctionValue := <span class="bu">make</span>(StateVector, <span class="dv">0</span>)</span>
<span id="cb7-15"><a aria-hidden="true" href="#cb7-15"></a>    <span class="kw">for</span> element = <span class="kw">range</span> stateVector {</span>
<span id="cb7-16"><a aria-hidden="true" href="#cb7-16"></a>        timeIncrement := ExpDistributedTimeIncrementFunction(</span>
<span id="cb7-17"><a aria-hidden="true" href="#cb7-17"></a>            smallTimeInterval,</span>
<span id="cb7-18"><a aria-hidden="true" href="#cb7-18"></a>        )</span>
<span id="cb7-19"><a aria-hidden="true" href="#cb7-19"></a>        <span class="co">// specify an arbitrary function of time for the event rate here</span></span>
<span id="cb7-20"><a aria-hidden="true" href="#cb7-20"></a>        eventRateLambda := EventRateLambdaFunction(timeStepNumber)</span>
<span id="cb7-21"><a aria-hidden="true" href="#cb7-21"></a>        prob := eventRateLambda /</span>
<span id="cb7-22"><a aria-hidden="true" href="#cb7-22"></a>            (eventRateLambda + <span class="dv">1</span><span class="fl">.0</span>/timeIncrement)</span>
<span id="cb7-23"><a aria-hidden="true" href="#cb7-23"></a>        <span class="co">// note that the difference between steps for a Poisson</span></span>
<span id="cb7-24"><a aria-hidden="true" href="#cb7-24"></a>        <span class="co">// process can only ever be 0 or 1</span></span>
<span id="cb7-25"><a aria-hidden="true" href="#cb7-25"></a>        element := <span class="dv">0</span><span class="fl">.0</span></span>
<span id="cb7-26"><a aria-hidden="true" href="#cb7-26"></a>        <span class="kw">if</span> rand.Float64() &lt; prob {</span>
<span id="cb7-27"><a aria-hidden="true" href="#cb7-27"></a>            element = <span class="dv">1</span><span class="fl">.0</span></span>
<span id="cb7-28"><a aria-hidden="true" href="#cb7-28"></a>        }</span>
<span id="cb7-29"><a aria-hidden="true" href="#cb7-29"></a>        sFunctionValue = <span class="bu">append</span>(sFunctionValue, element)</span>
<span id="cb7-30"><a aria-hidden="true" href="#cb7-30"></a>    }</span>
<span id="cb7-31"><a aria-hidden="true" href="#cb7-31"></a>    <span class="kw">return</span> sFunctionValue</span>
<span id="cb7-32"><a aria-hidden="true" href="#cb7-32"></a>}</span></code></pre>
  </div>
  <p>
   Note that this idea of integration kernels could also be applied back to our Wiener process. For example, another type of non-Markovian phenomenon that frequently arises across physical and life systems integrates the Wiener process history like so
   \[\begin{aligned}
S^{0}_{{\sf t}+1}(X',{\sf t}) &amp;= W^0_{{\sf t}+1}-W^0_{\sf t}\\
S^{1}_{{\sf t}+1}(X',{\sf t}) &amp;= \frac{1}{T}\sum^{{\sf t}}_{{\sf t}'=0}e^{-\frac{t({\sf t})-t({\sf t}')}{T}} (S^{0}_{{\sf t}'}-S^{0}_{{\sf t}'-1}) \,,\end{aligned}\tag{16}\]
   where
   <span class="math inline">
    \(T\)
   </span>
   is some decay coefficient which quantifies the length of memory in continuous time.
  </p>
  <p>
   So we’ve introduced the basic elements of our computational formalism and demonstrated how flexible the approach can be in simulating just about any stochastic phenomenon imaginable. Before progressing to algorithm design, it will be helpful to discuss some useful concepts that should enable us analyse the system later on in the book.
  </p>
  <h3 id="useful-probabilistic-concepts">
   Useful probabilistic concepts
  </h3>
  <p>
   The general stochastic process that we defined with Eq. (
   <a data-reference="1" data-reference-type="ref" href="#1">
    1
   </a>
   ) also has an implicit
   <em>
    master equation
   </em>
   associated to it which fully describes the time evolution of the
   <em>
    probability density function
   </em>
   <span class="math inline">
    \(P_{{\sf t}+1}(x)\)
   </span>
   of the most recent matrix row
   <span class="math inline">
    \(x=X_{{\sf t}+1}\)
   </span>
   at time
   <span class="math inline">
    \({\sf t}\)
   </span>
   . This can be written as
   \[\begin{aligned}
P_{{\sf t}+1}(x) &amp;= \frac{1}{{\sf t}}\sum_{{\sf t}'=0}^{{\sf t}}\int_{\omega_{{\sf t}'}}{\rm d}x' P_{{\sf t}'}(x') P_{({\sf t}+1){\sf t}'}(x\vert x') \label{eq:master-x-cont} \,,\end{aligned}\tag{17}\]
   where at the moment we are assuming the state space is continuous in each dimension and
   <span class="math inline">
    \(P_{({\sf t}+1){\sf t}'}(x\vert x')\)
   </span>
   is the conditional probability that the matrix row at time
   <span class="math inline">
    \(({\sf t}+1)\)
   </span>
   will be
   <span class="math inline">
    \(x=X_{{\sf t}+1}\)
   </span>
   given that the row at time
   <span class="math inline">
    \({\sf t}'\)
   </span>
   was
   <span class="math inline">
    \(x'=X_{{\sf t}'}\)
   </span>
   . This is a very general equation which should almost always apply to any continuous stochastic phenomenon we want to study in due course. To try and understand what this equation is saying I find it’s helpful to think of an iterative relationship between probabilities; each of which is connected by their relative conditional probabilities. This kind of thinking is also illustrated in Fig.
   <a data-reference="fig:master-eqn" data-reference-type="ref" href="#fig:master-eqn">
    1.2
   </a>
   .
  </p>
  <p>
   The factor of
   <span class="math inline">
    \(1/{\sf t}\)
   </span>
   in Eq. (
   <a data-reference="17" data-reference-type="ref" href="#17">
    17
   </a>
   ) is a normalisation factor — this just normalises the sum of all probabilities to 1 given that there is a sum over
   <span class="math inline">
    \({\sf t}'\)
   </span>
   . Note that, if the process is defined over continuous time, we would need to replace
   \[\begin{aligned}
\frac{1}{{\sf t}}\sum_{{\sf t}'=0}^{{\sf t}} \rightarrow \frac{1}{t({\sf t})}\sum_{{\sf t}'=0}^{{\sf t}}\delta t({\sf t}') \,.\end{aligned}\tag{18}\]
   But what is
   <span class="math inline">
    \(\omega_{\sf t}\)
   </span>
   ? You can think of this as just the domain of possible
   <span class="math inline">
    \(x'\)
   </span>
   inputs into the integral which will depend on the specific stochastic process we are looking at.
  </p>
  <figure>
   <img alt="" id="fig:master-eqn" src="images/master-eq-graph.drawio.png" style="width:8cm"/>
   <figcaption>
    Graph representation of Eq. (
    <a data-reference="17" data-reference-type="ref" href="#17">
     17
    </a>
    ).
   </figcaption>
  </figure>
  <p>
   What if we wanted the joint distribution of both rows
   <span class="math inline">
    \(P_{({\sf t}+1){\sf t}'}(x,x')\)
   </span>
   ? One way to obtain this would be to extend Eq. (
   <a data-reference="17" data-reference-type="ref" href="#17">
    17
   </a>
   ) such that both matrix rows are marginalised over separately like so
   \[\begin{aligned}
&amp;P_{({\sf t}+1){\sf t}'}(x,x') = \nonumber \\
&amp;\qquad \frac{1}{({\sf t}'-1){\sf t}}\sum_{{\sf t}''=0}^{{\sf t}}\sum_{{\sf t}'''=0}^{{\sf t}'-1}\int_{\omega_{{\sf t}''}}{\rm d}x''\int_{\omega_{{\sf t}'''}}{\rm d}x''' P_{{\sf t}''{\sf t}'''}(x'', x''') P_{({\sf t}+1){\sf t}''}(x\vert x'')P_{{\sf t}'{\sf t}'''}(x'\vert x''') \label{eq:joint-master-x-cont} \,.\end{aligned}\tag{19}\]
   Given Eqs. (
   <a data-reference="17" data-reference-type="ref" href="#17">
    17
   </a>
   ) and (
   <a data-reference="19" data-reference-type="ref" href="#19">
    19
   </a>
   ) it’s also possible to work out what the conditional probabilities would look like using the simple relation
   \[\begin{aligned}
P_{({\sf t}+1){\sf t}'}(x\vert x') &amp;= \frac{P_{({\sf t}+1){\sf t}'}(x,x')}{P_{{\sf t}'}(x')} \label{eq:cond-master-x-cont} \,.\end{aligned}\tag{20}\]
  </p>
  <p>
   The implicit notation in Eq. (
   <a data-reference="17" data-reference-type="ref" href="#17">
    17
   </a>
   ) can hide some staggering complexity. To analyse the system in more detail, we can also do a kind of Kramers-Moyal expansion
   <span class="citation" data-cites="kramers1940brownian moyal1949stochastic">
    [12], [13]
   </span>
   for each point in time to approximate the overall equation like this
   \[\begin{aligned}
P_{{\sf t}+1}(x) &amp;= \frac{1}{{\sf t}}\sum_{{\sf t}'=0}^{{\sf t}}P_{{\sf t}'}(x) - \frac{1}{{\sf t}}\sum_{{\sf t}'=0}^{{\sf t}}\sum_{i=1}^d\frac{\partial}{\partial x^i}\big[ \alpha^i_{({\sf t}+1){\sf t}'}(x)P_{{\sf t}'}(x)\big] \nonumber \\
&amp; \qquad + \frac{1}{2{\sf t}}\sum_{{\sf t}'=0}^{{\sf t}}\sum_{i=1}^d\sum_{j=1}^d\frac{\partial}{\partial x^i}\frac{\partial}{\partial x^j}\big[ \beta^{ij}_{({\sf t}+1){\sf t}'}(x)P_{{\sf t}'}(x)\big] + \dots \label{eq:master-x-cont-kramers-moyal} \,,\end{aligned}\tag{21}\]
   in which we have assumed that the state space is
   <span class="math inline">
    \(d\)
   </span>
   -dimensional. In this expansion, we also needed to define these new integrals
   \[\begin{aligned}
\alpha^i_{({\sf t}+1){\sf t}'}(x) &amp;=\int_{\omega_{{\sf t}'}} {\rm d}x'(x'-x)^iP_{({\sf t}+1){\sf t}'}(x'\vert x) \\
\beta^{ij}_{({\sf t}+1){\sf t}'}(x) &amp;= \int_{\omega_{{\sf t}'}} {\rm d}x'(x'-x)^i(x'-x)^jP_{({\sf t}+1){\sf t}'}(x'\vert x) \,.\end{aligned}\tag{22}\]
   So the matrix notation of Eq. (
   <a data-reference="17" data-reference-type="ref" href="#17">
    17
   </a>
   ) can indeed hide a very complicated calculation. Truncating the expansion at second-order, Eq. (
   <a data-reference="21" data-reference-type="ref" href="#21">
    21
   </a>
   ) tells us that there can be first and second derivatives contributing to the flow of probability to each element of the row
   <span class="math inline">
    \(x=X_{{\sf t}+1}\)
   </span>
   which depend on every element of the matrix
   <span class="math inline">
    \(X'\)
   </span>
   . The probability does indeed
   <em>
    flow
   </em>
   , in fact. We can define a quantity known as the ‘probability current’
   <span class="math inline">
    \(J_{({\sf t}+1){\sf t}'}(x)\)
   </span>
   from
   <span class="math inline">
    \({\sf t}'\)
   </span>
   to
   <span class="math inline">
    \(({\sf t}+1)\)
   </span>
   which illustrates this through the following continuity relation
   \[\begin{aligned}
P_{{\sf t}+1}(x) - \frac{1}{{\sf t}}\sum_{{\sf t}'=0}^{{\sf t}}P_{{\sf t}'}(x) = \frac{1}{{\sf t}}\sum_{{\sf t}'=0}^{{\sf t}}\big[ P_{{\sf t}+1}(x) - P_{{\sf t}'}(x)\big] = - \frac{1}{{\sf t}}\sum_{{\sf t}'=0}^{{\sf t}}J_{({\sf t}+1){\sf t}'}(x) \,.\end{aligned}\tag{23}\]
   By inspection of Eq. (
   <a data-reference="21" data-reference-type="ref" href="#21">
    21
   </a>
   ) we can therefore also deduce that
   \[\begin{aligned}
J^i_{({\sf t}+1){\sf t}'}(x) &amp;= \alpha^i_{({\sf t}+1){\sf t}'}(x)P_{{\sf t}'}(x) - \frac{1}{2}\sum_{j=1}^d\frac{\partial}{\partial x^j}\big[ \beta^{ij}_{({\sf t}+1){\sf t}'}(x)P_{{\sf t}'}(x)\big] + \dots \,.\end{aligned}\tag{24}\]
  </p>
  <p>
   What would happen if we assumed that
   <span class="math inline">
    \(\alpha\)
   </span>
   and
   <span class="math inline">
    \(\beta\)
   </span>
   were just arbitrary time-dependent functions? For example, let’s make the following assumptions
   \[\begin{aligned}
\alpha^i_{({\sf t}+1){\sf t}'}(x) &amp;= \mu^i({\sf t}')-x^i \\
\beta^{ij}_{({\sf t}+1){\sf t}'}(x) &amp;= 2\Sigma^{ij}(\theta, {\sf t}') \,,\end{aligned}\tag{25}\]
   where
   <span class="math inline">
    \(\mu ({\sf t}')\)
   </span>
   is an arbitrary vector-valued function of the timestep and
   <span class="math inline">
    \(\Sigma (\theta ,{\sf t}')\)
   </span>
   is an arbitrary matrix (often known as the ‘diffusion tensor’) which depends on both the timestep and a set of hyperparameters
   <span class="math inline">
    \(\theta\)
   </span>
   . If we now also assume stationarity of
   <span class="math inline">
    \(P_{{\sf t}'}(x)=P_{{\sf t}''}(x)\)
   </span>
   for any
   <span class="math inline">
    \({\sf t}'\)
   </span>
   and
   <span class="math inline">
    \({\sf t}''\)
   </span>
   such that
   \[\begin{aligned}
P_{{\sf t}+1}(x) = \frac{1}{{\sf t}}\sum_{{\sf t}'=0}^{{\sf t}} P_{{\sf t}'}(x) \,,\end{aligned}\tag{26}\]
   we can solve Eq. (
   <a data-reference="21" data-reference-type="ref" href="#21">
    21
   </a>
   ) to obtain the following stationary solution
   \[\begin{aligned}
P_{{\sf t}'}(x) &amp;= {\sf MultivariateNormalPDF}[x;\mu ({\sf t}'),\Sigma (\theta , {\sf t}')]\label{eq:stat-sol-kramers-moyal}\,.\end{aligned}\tag{27}\]
   I actually hid a little bit of the detail in that last step; the solution also required the identification that the flow of probability between timesteps vanishes uniquely for each and every
   <span class="math inline">
    \({\sf t}'\)
   </span>
   such that
   <span class="math inline">
    \(J_{({\sf t}+1){\sf t}'}(x)=0\)
   </span>
   .
  </p>
  <figure>
   <img alt="" id="fig:stat-sol-kramers-moyal-cond" src="images/gp-like-diag.drawio.png" style="width:8cm"/>
   <figcaption>
    Graph representation of the generative process implied by Eq. (
    <a data-reference="28" data-reference-type="ref" href="#28">
     28
    </a>
    ).
   </figcaption>
  </figure>
  <p>
   It’s possible to take this derivation a bit further by expanding Eq. (
   <a data-reference="19" data-reference-type="ref" href="#19">
    19
   </a>
   ) in a similar fashion, truncating it to second-order, assuming only time-dependent terms and then solving it in the stationary limit. By plugging this solution (and its corresponding marginal distribution equivalent) into Eq. (
   <a data-reference="20" data-reference-type="ref" href="#20">
    20
   </a>
   ), it’s possible to get something that looks like this conditional distribution
   \[\begin{aligned}
P_{({\sf t}+1){\sf t}'}(x\vert x') &amp;\propto {\rm exp}\bigg\{ -\frac{1}{2}\sum^d_{i=1}\sum^d_{j=1}\big[ x-f({\sf t}+1)\big]^i [K^{-1}(\theta , {\sf t}+1,{\sf t}+1)]^{ij}\big[ x-f({\sf t}+1)\big]^j \nonumber \\
&amp; \qquad \qquad + \sum^d_{i=1}\sum^d_{j=1}\big[ x-f({\sf t}+1)\big]^i [K^{-1}(\theta , {\sf t}+1,{\sf t}')]^{ij}\big[ x'-f({\sf t}')\big]^j  \bigg\} \label{eq:stat-sol-kramers-moyal-cond}\,,\end{aligned}\tag{28}\]
   where
   <span class="math inline">
    \(K(\theta , {\sf t}+1,{\sf t}')\)
   </span>
   is some arbitrary covariance matrix that encodes how the correlation structure varies with the between compared states at two different timesteps and
   <span class="math inline">
    \(K^{-1}(\theta , {\sf t}+1,{\sf t}')\)
   </span>
   denotes taking its inverse. Eq. (
   <a data-reference="28" data-reference-type="ref" href="#28">
    28
   </a>
   ) may look a bit familiar to some readers who like using Gaussian processes from the machine learning literature
   <span class="citation" data-cites="murphy2012machine">
    [14]
   </span>
   — this version implies a
   <em>
    generative
   </em>
   model for a future
   <span class="math inline">
    \(x\)
   </span>
   value (which I’ve illustrated in Fig
   <a data-reference="fig:stat-sol-kramers-moyal-cond" data-reference-type="ref" href="#fig:stat-sol-kramers-moyal-cond">
    1.3
   </a>
   ), in contrast to the more standard equation used to
   <em>
    infer
   </em>
   values of
   <span class="math inline">
    \(f\)
   </span>
   . These are two sides of the same coin though.
  </p>
  <p>
   What other processes can be described by Eq. (
   <a data-reference="17" data-reference-type="ref" href="#17">
    17
   </a>
   )? For Markovian phenomena, the equation no longer depends on timesteps older than the immediately previous one, hence the expression reduces to just
   \[\begin{aligned}
P_{{\sf t}+1}(x) &amp;= \int_{\omega_{\sf t}}{\rm d}x' P_{\sf t}(x') P_{({\sf t}+1){\sf t}}(x\vert x') \label{eq:master-x-cont-markov} \,.\end{aligned}\tag{29}\]
   It’s also easy to show that Eq. (
   <a data-reference="21" data-reference-type="ref" href="#21">
    21
   </a>
   ) naturally simplifies into the more usually applied Kramers-Moyal expansion when considering a Markovian process — you just remove the sum over
   <span class="math inline">
    \({\sf t}'\)
   </span>
   and the
   <span class="math inline">
    \(1/{\sf t}\)
   </span>
   normalisation factor.
  </p>
  <p>
   Note that an analog of Eq. (
   <a data-reference="17" data-reference-type="ref" href="#17">
    17
   </a>
   ) exists for discrete state spaces as well. We just need to replace the integral with a sum and the schematic would look something like this
   \[\begin{aligned}
P_{{\sf t}+1}(x) &amp;= \frac{1}{{\sf t}}\sum_{{\sf t}'=0}^{\sf t}\sum_{\omega_{{\sf t}'}} P_{{\sf t}'}(x') P_{({\sf t}+1){\sf t}'}(x \vert x') \label{eq:master-x-disc} \,,\end{aligned}\tag{30}\]
   where we note that the
   <span class="math inline">
    \(P\)
   </span>
   ’s in the expression above all now refer to
   <em>
    probability mass functions
   </em>
   . Because the state space is now discrete, we cannot immediately intuit an approximative expansion from this expression.
  </p>
  <p>
   Ok, as a brief mathematical aside; if one is really determined to use a similar approach to the one we derived above, it’s quite straightforward to rewrite it in terms of continuous-valued characteristic functions like so
   \[\begin{aligned}
\varphi_{{\sf t}+1}(s) &amp;= \frac{1}{{\sf t}}\sum_{{\sf t}'=0}^{\sf t}\int_{\ell_{{\sf t}'}}{\rm d}s' {\cal C}(s') \varphi_{{\sf t}'}(s') \varphi_{({\sf t}+1){\sf t}'}(s \vert s') \label{eq:master-x-disc-char} \\
{\cal C}(s') &amp;= \frac{1}{(2\pi )^d}\sum_{\omega_{{\sf t}'}}e^{-i(s'\cdot x')} \,,\end{aligned}\tag{31}\]
   where
   <span class="math inline">
    \(\ell_{{\sf t}'}\)
   </span>
   defines all the continuous values that the vector
   <span class="math inline">
    \(s'\)
   </span>
   can possibly have at time
   <span class="math inline">
    \({\sf t}'\)
   </span>
   . In the expression above,
   <span class="math inline">
    \({\cal C}(s')\)
   </span>
   acts like is a kind of comb
   <a class="footnote-ref" href="#fn5" id="fnref5" role="doc-noteref">
    <sup>
     5
    </sup>
   </a>
   to map the continuous frequency domain of
   <span class="math inline">
    \(s'\)
   </span>
   onto the discrete state space of
   <span class="math inline">
    \(x'\)
   </span>
   . Note also that
   <span class="math inline">
    \({\cal C}(s')\)
   </span>
   uses the imaginary number
   <span class="math inline">
    \(i\)
   </span>
   and, to be visually tidier, the dot product notation
   <span class="math inline">
    \(a\cdot b\)
   </span>
   just means the sum of vector elements:
   <span class="math inline">
    \(a\cdot b = \sum_{\forall k}a^kb^k\)
   </span>
   . In principle, one can perform an approximative expansion on Eq. (
   <a data-reference="31" data-reference-type="ref" href="#31">
    31
   </a>
   ) like we did for continuous state spaces. This isn’t always the most practical way of analysing the system though.
  </p>
  <p>
   We have one more important example to discuss and then we can cap off this analysis subsection. In the even-simpler case where
   <span class="math inline">
    \(x\)
   </span>
   is just a vector of binary ‘on’ or ‘off’ states, Eq. (
   <a data-reference="30" data-reference-type="ref" href="#30">
    30
   </a>
   ) reduces to
   \[\begin{aligned}
P^i_{{\sf t}+1} &amp;= \frac{1}{{\sf t}}\sum_{{\sf t}'=0}^{\sf t} \sum_{j=1}^d P^j_{{\sf t}'} P^{ij}_{({\sf t}+1){\sf t}'} = \frac{1}{{\sf t}}\sum_{{\sf t}'=0}^{\sf t} \sum_{j=1}^d \big[ P^j_{{\sf t}'} A^{ij}_{({\sf t}+1){\sf t}'} + (1-P^j_{{\sf t}'}) B^{ij}_{({\sf t}+1){\sf t}'} \big] \label{eq:master-x-disc-binary}\,,\end{aligned}\tag{32}\]
   where
   <span class="math inline">
    \(P^i_{{\sf t}'}\)
   </span>
   now represents the probability that element
   <span class="math inline">
    \(x^i=1\)
   </span>
   (is ‘on’) at time
   <span class="math inline">
    \({\sf t}'\)
   </span>
   . The matrices
   <span class="math inline">
    \(A\)
   </span>
   and
   <span class="math inline">
    \(B\)
   </span>
   are defined as conditional probabilities where the previous state in time
   <span class="math inline">
    \(P^j_{{\sf t}'}\)
   </span>
   was either ‘on’ or ‘off’, respectively.
  </p>
  <h3 id="software-design">
   Software design
  </h3>
  <p>
   So I’ve proposed a computational formalism and done a bit of analysis on it to demonstrate that it can cope with a variety of different stochastic phenomena. Now I think we’re ready to summarise what we want the stochadex software package to be able to do. But what’s so complicated about Eq. (
   <a data-reference="1" data-reference-type="ref" href="#1">
    1
   </a>
   )? Can’t we just implement an iterative algorithm with a single function? It’s true that the fundamental concept is very straightforward, but as I’ll discuss in due course; the stochadex needs to have a lot of configurable features so that it’s applicable in different situations. Ideally, the stochadex sampler should be designed to try and maintain a balance between performance and flexibility of utilisation.
  </p>
  <p>
   If we begin with the obvious first set of criteria; we want to be able to freely configure the iteration function
   <span class="math inline">
    \(F\)
   </span>
   of Eq. (
   <a data-reference="1" data-reference-type="ref" href="#1">
    1
   </a>
   ) and the timestep function
   <span class="math inline">
    \(t\)
   </span>
   of Eq. (
   <a data-reference="2" data-reference-type="ref" href="#2">
    2
   </a>
   ) so that any process we want can be described. The point at which a simulation stops can also depend on some algorithm termination condition which the user should be able to specify up-front.
  </p>
  <p>
   Once someone has written the code to create these functions for the stochadex, I want to then be able to recall them in future only with configuration files while maintaining the possibility of changing their simulation run parameters. This flexibility should facilitate our uses for the simulation later in the book, and from this perspective it also makes sense that the parameters should include the random seed and initial state value.
  </p>
  <p>
   The state history matrix
   <span class="math inline">
    \(X\)
   </span>
   should be configurable in terms of its number of rows — what we’ll call the ‘state width’ — and its number of columns — what we’ll call the ‘state history depth’. If we were to keep increasing the state width up to millions of elements or more, it’s likely that on most machines the algorithm performance would grind to a halt when trying to iterate over the resulting
   <span class="math inline">
    \(X\)
   </span>
   within a single thread. Hence, before the algorithm or its performance in any more detail, we can pre-empt the requirement that
   <span class="math inline">
    \(X\)
   </span>
   should represented in computer memory by a set of partitioned matrices which are all capable of communicating to one-another downstream. In this paradigm, I’d like the user to be able to configure which state partitions are able to communicate with each other without having to write any new code.
  </p>
  <p>
   For convenience, it seems sensible to also make the outputs from stochadex runs configurable. A user should be able to change the form of output that they want through, e.g., some specified function of
   <span class="math inline">
    \(X\)
   </span>
   at the time of outputting data. The times that the stochadex should output this data can also be decided by some user-specified condition so that the frequency of output is fully configurable as well.
  </p>
  <figure>
   <img alt="" id="fig:data-types-design" src="images/stochadex-data-types.drawio.png" style="width:15cm"/>
   <figcaption>
    A relational summary of the core data types in the stochadex.
   </figcaption>
  </figure>
  <p>
   In summary, I’ve put together a schematic of data types and their relationships in Fig.
   <a data-reference="fig:data-types-design" data-reference-type="ref" href="#fig:data-types-design">
    1.4
   </a>
   . In this diagram there is some indication of the data type that I propose to store each piece information in (in Go syntax), and the diagram as a whole should serve as a useful guide to the basic structure of configuration files for the stochadex.
  </p>
  <p>
   It’s clear that in order to simulate Eq. (
   <a data-reference="1" data-reference-type="ref" href="#1">
    1
   </a>
   ), we need an interative algorithm which reapplies a user-specified function to the continually-updated history. But let’s now return to the point I made earlier about how the performance of such an algorithm will depend on the size of the state history matrix
   <span class="math inline">
    \(X\)
   </span>
   . The key bit of the algorithm design that isn’t so straightforward is: how do we sucessfully split this state history up into separate partitions in memory while still enabling them to communicate effectively with each other? Other generalised simulation frameworks — such as SimPy
   <span class="citation" data-cites="simpy">
    [16]
   </span>
   , StoSpa
   <span class="citation" data-cites="stospa">
    [17]
   </span>
   and FLAME GPU
   <span class="citation" data-cites="flamegpu">
    [18]
   </span>
   — have all approached this problem in different ways, and with different software architectures.
  </p>
  <p>
   In Fig.
   <a data-reference="fig:loop-design" data-reference-type="ref" href="#fig:loop-design">
    1.5
   </a>
   I’ve illustrated what a loop involving two separate state partitions looks like in the stochadex simulator. Each parition is handled by concurrently running execution threads of the same process, while a separate process is used to handle the outputs from the algorithm. While this diagram illustrates only a single use for multiple processes, it’s obviously true that we may run many of these whole diagrams at once on a multicore machine to generate a batch of independent ensembles if necessary.
  </p>
  <figure>
   <img alt="" id="fig:loop-design" src="images/stochadex-loop.drawio.png" style="width:15cm"/>
   <figcaption>
    A loop of the stochadex simulation algorithm with two state partitions.
   </figcaption>
  </figure>
  <h3 id="implementation-details">
   Implementation details
  </h3>
  <p>
   Now that the design of the algorithm and its basic data types have been outlined, we can finally turn to the implementation details. As I mentioned in the introduction, most of the core software in this book has been written in Go — the main reason for this is mostly because it is a performant language that I enjoy developing with. More objectively, the key feature of Go that made it attractive for building the stochadex are its lightweight concurrency primitives called ‘goroutines’
   <span class="citation" data-cites="goroutines">
    [19]
   </span>
   which made it easy and quick to piece together a scalable pipeline without excessive head-scratching. Ideal for projects in your limited free time!
  </p>
  <h2 id="simulating-a-financial-market">
   Simulating a financial market
  </h2>
  <p>
   <span>
    <strong>
     Concept.
    </strong>
   </span>
   The idea here is to use the Q-Hawkes processes and the Bouchaud work to come up with some interesting simulations of financial markets.
  </p>
  <h3 id="introducing-q-hawkes-processes">
   Introducing Q-Hawkes processes
  </h3>
  <h2 id="quantum-jumps-on-generic-networks">
   Quantum jumps on generic networks
  </h2>
  <p>
   <span>
    <strong>
     Concept.
    </strong>
   </span>
   The idea is to follow this sort of thing
   <a href="https://en.wikipedia.org/wiki/Quantum_jump_method">
    here
   </a>
   to simulate the Lindblad equation over an arbitrary network of entangled states.
  </p>
  <h3 id="the-lindblad-equation">
   The Lindblad equation
  </h3>
  <h1 id="part-2.-how-do-we-then-learnidentify-the-answer-to-part-1-from-real-world-data">
   Part 2.
   <span>
    How do we then learn/identify the answer to Part 1 from real-world data?
   </span>
  </h1>
  <h2 id="empirical-dynamical-emulators">
   Empirical dynamical emulators
  </h2>
  <p>
   <span>
    <strong>
     Concept.
    </strong>
   </span>
   The idea here is to leverage our probabilistic formalism to be able to empirically emulate a wide variety stochastic phenomena within a generalised framework.
  </p>
  <h2 id="inferring-dynamical-2d-maps">
   Inferring dynamical 2D maps
  </h2>
  <p>
   <span>
    <strong>
     Concept.
    </strong>
   </span>
   The idea here is
  </p>
  <h3 id="adapting-the-stochadex-formalism">
   Adapting the stochadex formalism
  </h3>
  <h2 id="learning-from-ants-on-curved-surfaces">
   Learning from ants on curved surfaces
  </h2>
  <p>
   <span>
    <strong>
     Concept.
    </strong>
   </span>
   The idea here is
  </p>
  <h3 id="diffusive-limits-for-ant-interactions">
   Diffusive limits for ant interactions
  </h3>
  <h2 id="hydrodynamic-ensembles-from-input-data">
   Hydrodynamic ensembles from input data
  </h2>
  <p>
   <span>
    <strong>
     Concept.
    </strong>
   </span>
   The idea here is
  </p>
  <h3 id="the-boltzmannnavier-stokes-equations">
   The Boltzmann/Navier-Stokes equations
  </h3>
  <h2 id="generalised-statistical-inference-tools">
   Generalised statistical inference tools
  </h2>
  <p>
   <span>
    <strong>
     Concept.
    </strong>
   </span>
   The idea here is to extend the stochadex with tools for very generalised statistical inference (ABC algorithms and the like) that will work in nearly every situation. Probably need to exploit the phase space analogy of the formalism.
  </p>
  <h3 id="likelihood-free-methods">
   Likelihood-free methods
  </h3>
  <h1 id="part-3.-how-do-we-simulate-a-general-set-of-control-policies-to-interact-with-the-answer-to-part-1">
   Part 3.
   <span>
    How do we simulate a general set of control policies to interact with the answer to Part 1?
   </span>
  </h1>
  <h2 id="interacting-with-systems-in-general">
   Interacting with systems in general
  </h2>
  <p>
   <span>
    <strong>
     Concept.
    </strong>
   </span>
   The idea here is
  </p>
  <h3 id="parameterising-general-interactions">
   Parameterising general interactions
  </h3>
  <h2 id="angling-for-freshwater-fish">
   Angling for freshwater fish
  </h2>
  <p>
   <span>
    <strong>
     Concept.
    </strong>
   </span>
   The idea here is
  </p>
  <h3 id="a-large-scale-lotka-volterra-model">
   A large-scale Lotka-Volterra model
  </h3>
  <p>
   Inspired by the empirical dynamical modelling approach to sockeye salmon in Ref.
   <span class="citation" data-cites="ye2015equation">
    [20]
   </span>
   , but also desiring a generative model which has some link to the classic causal models promoted by mathematical ecology; the goal here is to create and calibrate a stochastic model which predicts the fish counts, weights, lengths and ages for each species in each area based on the past system states. To do this, we will combine some well-known models from mathematical ecology with supervised learning.
  </p>
  <p>
   The one-step master equation for the proposed stochastic simulation is given implictly by
  </p>
  <p>
   \[\begin{aligned}
\frac{{\rm d}}{{\rm d} t} P(\dots, n_{i}, \dots, t) &amp;= \sum_{\forall i}{\cal T}^{+}_{i}(\dots, n_i-1, \dots, {\sf f}, t)P(\dots, n_{i}-1, \dots, t) \\
&amp;+ \sum_{\forall i}{\cal T}^{-}_{i}(\dots, n_i+1, \dots, {\sf f}, t)P(\dots, n_{i}+1, \dots, t) \\
&amp;- \sum_{\forall i}\bigg[ {\cal T}^{+}_{i}(\dots, n_i, \dots, {\sf f}, t) + {\cal T}^{-}_{i}(\dots, n_i, \dots, {\sf f}, t) \bigg] P(\dots, n_{i}, \dots,t) \,,\end{aligned}\tag{33}\]
  </p>
  <p>
   where the time
   <span class="math inline">
    \(t\)
   </span>
   is defined in units of years and
   <span class="math inline">
    \({\cal T}^{+}_{i}\)
   </span>
   and
   <span class="math inline">
    \({\cal T}^{-}_{i}\)
   </span>
   are the transition coefficients for the
   <span class="math inline">
    \(i\)
   </span>
   -th species, which depend not only on the counts for all species
   <span class="math inline">
    \(n_1, n_2, \dots\)
   </span>
   , but also (in principle) on a larger feature space
   <span class="math inline">
    \({\sf f}\)
   </span>
   generated by the available data up to time
   <span class="math inline">
    \(t\)
   </span>
   .
  </p>
  <p>
   The famous Lotka-Volterra system, with some modficiations for fishing and a larger set of species, would suggest transition coefficients of the form
  </p>
  <p>
   \[\begin{aligned}
{\cal T}^{+}_{i}(\dots, n_i, \dots, {\sf f}, t) = {\cal T}^{+}_{i}(\dots, n_i, \dots) &amp;= \Lambda_{i}(n_{i}) + n_{i}\alpha_{i}\sum_{\forall i' \, {\sf prey}}n_{i'}\\
{\cal T}^{-}_{i}(\dots, n_i, \dots, {\sf f}, t) = {\cal T}^{-}_{i}(\dots, n_i, \dots) &amp;= n_{i}\mu_{i} +  n_{i}\gamma_{i} + n_{i}\beta_{i} \sum_{\forall i' \, {\sf pred}} n_{i'} \,,\end{aligned}\tag{34}\]
  </p>
  <p>
   where:
   <span class="math inline">
    \(\Lambda_{i}(n_{i}) = \tilde{\Lambda_{i}}n_{i}e^{-\lambda_i(n_{i}-1)}\)
   </span>
   is the density-dependent birth rate;
   <span class="math inline">
    \(\mu_{i}\)
   </span>
   is the species death rate;
   <span class="math inline">
    \(\alpha_{i}\)
   </span>
   is the increase in the baseline birth rate per fish caused by the increase in prey population;
   <span class="math inline">
    \(\beta_{i}\)
   </span>
   is the rate per fish of predation of the species; and
   <span class="math inline">
    \(\gamma_{i}\)
   </span>
   accounts for the rate of recreational fishing per fish of the species. To approach the present data-driven simulation problem, we’re going to generalise this model by training
   <span class="math inline">
    \({\cal T}^{+}_{i}(\dots, n_i, \dots, {\sf f}, t)\)
   </span>
   and
   <span class="math inline">
    \({\cal T}^{-}_{i}(\dots, n_i, \dots, {\sf f}, t)\)
   </span>
   directly from the data and generated features.
  </p>
  <p>
   Look into the likelihood from, e.g., an electrofishing survey such as in Ref.
   <span class="citation" data-cites="envagency2015">
    [21]
   </span>
   ...
  </p>
  <p>
   \[\begin{aligned}
{\sf Likelihood} &amp;= \sum_{{\sf data}}{\rm NB}\big[{\sf data};w_{i,{\sf survey}}\langle n_i(t_{{\sf data}})\rangle,k_{i,{\sf survey}}\big] \,,\end{aligned}\tag{35}\]
  </p>
  <h2 id="managing-a-rugby-match">
   Managing a Rugby match
  </h2>
  <p>
   <span>
    <strong>
     Concept.
    </strong>
   </span>
   The idea here is
  </p>
  <h3 id="introduction-1">
   Introduction
  </h3>
  <p>
   Since the basic game engine will run using the
   <a href="https://github.com/umbralcalc/stochadex">
    stochadex
   </a>
   sampler, the novelties in this project are all in the design of the rugby match model itself. And, in this instance, I’m not especially keen on spending a lot of time doing detailed data analysis to come up with the most realistic values for the parameters that are dreamed up here. Even though this would also be interesting.
  </p>
  <p>
   One could do this data analysis, for instance, by scraping player-level performance data from one of the excellent websites that collect live commentary data such as
   <a href="https://www.rugbypass.com/">
    rugbypass.com
   </a>
   or
   <a href="https://www.espn.co.uk/rugby/">
    espn.co.uk/rugby
   </a>
   .
  </p>
  <p>
   This game is primarily a way of testing out the interface of the stochadex for other users to build projects with. This should help to both iron out some of the kinks in the design, as well as prioritise adding some more convenience methods for event-based modelling into its code base.
  </p>
  <h3 id="designing-the-event-simulation-engine">
   Designing the event simulation engine
  </h3>
  <p>
   We need to begin by specifying an appropriate event space to live in when simulating a rugby match. It is important at this level that events are defined in quite broadly applicable terms, as it will define the state space available to our stochastic sampler and hence the simulated game will never be allowed to exist outside of it. So, in order to capture the fully detailed range of events that are possible in a real-world match, we will need to be a little imaginative in how we define certain gameplay elements when we move through the space.
  </p>
  <p>
   The diagrams below sum up what should hopefully work as a decent initial approximation while providing a little context with specific examples of play action.
  </p>
  <figure>
   <img alt="" id="fig:event-graph" src="images/test.drawio.png" style="width:8cm"/>
   <figcaption>
    Simplified event graph of a rugby union match - replace with drawio.
   </figcaption>
  </figure>
  <figure>
   <img alt="" id="fig:model-ideas" src="images/test.drawio.png" style="width:10cm"/>
   <figcaption>
    Optional model ideas - replace with drawio.
   </figcaption>
  </figure>
  <h3 id="linking-to-player-attributes">
   Linking to player attributes
  </h3>
  <h3 id="deciding-on-gameplay-actions">
   Deciding on gameplay actions
  </h3>
  <h3 id="writing-the-game-itself">
   Writing the game itself
  </h3>
  <h2 id="influencing-house-prices">
   Influencing house prices
  </h2>
  <p>
   <span>
    <strong>
     Concept.
    </strong>
   </span>
   The idea here is
  </p>
  <h1 id="part-4.-how-do-we-then-optimise-the-answer-to-part-3-to-achieve-a-specified-control-objective">
   Part 4.
   <span>
    How do we then optimise the answer to Part 3 to achieve a specified control objective?
   </span>
  </h1>
  <h2 id="optimising-actions-for-control-objectives">
   Optimising actions for control objectives
  </h2>
  <p>
   <span>
    <strong>
     Concept.
    </strong>
   </span>
   The idea controlhere is
  </p>
  <h2 id="resource-allocation-for-epidemics">
   Resource allocation for epidemics
  </h2>
  <p>
   <span>
    <strong>
     Concept.
    </strong>
   </span>
   The idea here is to limit the spread of some abstract epidemic through the correct time-dependent resource allocation.
  </p>
  <h2 id="quantum-system-control">
   Quantum system control
  </h2>
  <p>
   <span>
    <strong>
     Concept.
    </strong>
   </span>
   The idea here is to follow stuff along these lines
   <a href="https://arxiv.org/pdf/1210.7127.pdf">
    here
   </a>
   .
  </p>
  <h2 id="other-models">
   Other models
  </h2>
  <p>
   <span>
    <strong>
     Concept.
    </strong>
   </span>
   The idea here is
  </p>
  <div class="references" id="refs" role="doc-bibliography">
   <div id="ref-golang">
    <p>
     [1] “The Go Programming Language.”
     <a href="https://go.dev/">
      https://go.dev/
     </a>
     .
    </p>
   </div>
   <div id="ref-pythonlang">
    <p>
     [2] “The Python Programming Language.”
     <a href="https://www.python.org/">
      https://www.python.org/
     </a>
     .
    </p>
   </div>
   <div id="ref-diffusingideasbookgithub">
    <p>
     [3] “The Diffusing Ideas GitHub Repository.”
     <a href="https://github.com/umbralcalc/diffusing-ideas">
      https://github.com/umbralcalc/diffusing-ideas
     </a>
     .
    </p>
   </div>
   <div id="ref-mitlicense">
    <p>
     [4] “Open Source Initiative: MIT License.”
     <a href="https://opensource.org/licenses/MIT">
      https://opensource.org/licenses/MIT
     </a>
     .
    </p>
   </div>
   <div id="ref-van1992stochastic">
    <p>
     [5] N. G. Van Kampen,
     <em>
      Stochastic processes in physics and chemistry
     </em>
     , vol. 1. Elsevier, 1992.
    </p>
   </div>
   <div id="ref-risken1996fokker">
    <p>
     [6] H. Risken, “Fokker-planck equation,” in
     <em>
      The fokker-planck equation
     </em>
     , Springer, 1996, pp. 63–95.
    </p>
   </div>
   <div id="ref-rog-will-2000">
    <p>
     [7] L. Rogers and D. Williams, “Diffusions, markov processes and martingales 2: Ito calculus,” vol. 1, Cambridge University Press, 2000, p. xiv+480. doi:
     <a href="https://doi.org/10.1017/CBO9781107590120">
      10.1017/CBO9781107590120
     </a>
     .
    </p>
   </div>
   <div id="ref-decreusefond1999stochastic">
    <p>
     [8] L. Decreusefond and others, “Stochastic analysis of the fractional brownian motion,”
     <em>
      Potential analysis
     </em>
     , vol. 10, no. 2, pp. 177–214, 1999.
    </p>
   </div>
   <div id="ref-gillespie1977exact">
    <p>
     [9] D. T. Gillespie, “Exact stochastic simulation of coupled chemical reactions,”
     <em>
      The journal of physical chemistry
     </em>
     , vol. 81, no. 25, pp. 2340–2361, 1977.
    </p>
   </div>
   <div id="ref-neyman1958statistical">
    <p>
     [10] J. Neyman and E. L. Scott, “Statistical approach to problems of cosmology,”
     <em>
      Journal of the Royal Statistical Society: Series B (Methodological)
     </em>
     , vol. 20, no. 1, pp. 1–29, 1958.
    </p>
   </div>
   <div id="ref-hawkes1971spectra">
    <p>
     [11] A. G. Hawkes, “Spectra of some self-exciting and mutually exciting point processes,”
     <em>
      Biometrika
     </em>
     , vol. 58, no. 1, pp. 83–90, 1971.
    </p>
   </div>
   <div id="ref-kramers1940brownian">
    <p>
     [12] H. A. Kramers, “Brownian motion in a field of force and the diffusion model of chemical reactions,”
     <em>
      Physica
     </em>
     , vol. 7, no. 4, pp. 284–304, 1940.
    </p>
   </div>
   <div id="ref-moyal1949stochastic">
    <p>
     [13] J. Moyal, “Stochastic processes and statistical physics,”
     <em>
      Journal of the Royal Statistical Society. Series B (Methodological)
     </em>
     , vol. 11, no. 2, pp. 150–210, 1949.
    </p>
   </div>
   <div id="ref-murphy2012machine">
    <p>
     [14] K. P. Murphy,
     <em>
      Machine learning: A probabilistic perspective
     </em>
     . MIT press, 2012.
    </p>
   </div>
   <div id="ref-brandwood2012fourier">
    <p>
     [15] D. Brandwood,
     <em>
      Fourier transforms in radar and signal processing
     </em>
     . Artech House, 2012.
    </p>
   </div>
   <div id="ref-simpy">
    <p>
     [16] “SimPy: a process-based discrete-event simulation framework.”
     <a href="https://gitlab.com/team-simpy/simpy/">
      https://gitlab.com/team-simpy/simpy/
     </a>
     .
    </p>
   </div>
   <div id="ref-stospa">
    <p>
     [17] “StoSpa: A C++ package for running stochastic simulations to generate sample paths for reaction-diffusion master equation.”
     <a href="https://github.com/BartoszBartmanski/StoSpa">
      https://github.com/BartoszBartmanski/StoSpa
     </a>
     .
    </p>
   </div>
   <div id="ref-flamegpu">
    <p>
     [18] “FLAME GPU: A GPU accelerated agent-based simulation library for domain independent complex systems simulation.”
     <a href="https://github.com/FLAMEGPU/FLAMEGPU2/">
      https://github.com/FLAMEGPU/FLAMEGPU2/
     </a>
     .
    </p>
   </div>
   <div id="ref-goroutines">
    <p>
     [19] “A Tour of Go: Goroutines.”
     <a href="https://go.dev/tour/concurrency/1">
      https://go.dev/tour/concurrency/1
     </a>
     .
    </p>
   </div>
   <div id="ref-ye2015equation">
    <p>
     [20] H. Ye
     <em>
      et al.
     </em>
     , “Equation-free mechanistic ecosystem forecasting using empirical dynamic modeling,”
     <em>
      Proceedings of the National Academy of Sciences
     </em>
     , vol. 112, no. 13, pp. E1569–E1576, 2015.
    </p>
   </div>
   <div id="ref-envagency2015">
    <p>
     [21] “Electrofishing to assess a river’s health.”
     <a href="https://environmentagency.blog.gov.uk/2015/10/29/electrofishing-to-assess-a-rivers-health/">
      https://environmentagency.blog.gov.uk/2015/10/29/electrofishing-to-assess-a-rivers-health/
     </a>
     .
    </p>
   </div>
  </div>
  <section class="footnotes" role="doc-endnotes">
   <hr/>
   <ol>
    <li id="fn1" role="doc-endnote">
     <p>
      For example, I’ll typically be thinking more in terms of ‘matrices’ and less about ‘operators’.
      <a class="footnote-back" href="#fnref1" role="doc-backlink">
       ↩︎
      </a>
     </p>
    </li>
    <li id="fn2" role="doc-endnote">
     <p>
      The repositories will always be somewhere on this list:
      <a href="https://github.com/umbralcalc?tab=repositories">
       https://github.com/umbralcalc?tab=repositories
      </a>
      .
      <a class="footnote-back" href="#fnref2" role="doc-backlink">
       ↩︎
      </a>
     </p>
    </li>
    <li id="fn3" role="doc-endnote">
     <p>
      Or memory at least within some window.
      <a class="footnote-back" href="#fnref3" role="doc-backlink">
       ↩︎
      </a>
     </p>
    </li>
    <li id="fn4" role="doc-endnote">
     <p>
      Which would implictly give
      <span class="math inline">
       \(S^{i}_{{\sf t}+1}(X',{\sf t}) = (X^i_{{\sf t}+1}+X^i_{\sf t})(W^i_{{\sf t}+1}-W^i_{\sf t}) / 2\)
      </span>
      for Eq. (
      <a data-reference="6" data-reference-type="ref" href="#6">
       6
      </a>
      ).
      <a class="footnote-back" href="#fnref4" role="doc-backlink">
       ↩︎
      </a>
     </p>
    </li>
    <li id="fn5" role="doc-endnote">
     <p>
      This is very similar to how a ‘Dirac comb’ works in signal processing
      <span class="citation" data-cites="brandwood2012fourier">
       [15]
      </span>
      .
      <a class="footnote-back" href="#fnref5" role="doc-backlink">
       ↩︎
      </a>
     </p>
    </li>
   </ol>
  </section>
  <script>
   // Used to toggle the menu on small screens when clicking on the menu button
function myFunction() {
  var x = document.getElementById("navDemo");
  if (x.className.indexOf("w3-show") == -1) {
    x.className += " w3-show";
  } else { 
    x.className = x.className.replace(" w3-show", "");
  }
}
  </script>
 </body>
</html>