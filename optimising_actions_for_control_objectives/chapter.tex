\chapter{\sffamily Optimising actions for control objectives}

{\bfseries\sffamily Concept.} The idea here is 

\section{\sffamily States, actions and rewards}

We observe the state at timestep ${\sf t}$ with $S_{{\sf t}}(X_{{\sf t}}, {\sf t})$. In a Markov Decision Process (MDP), based on this observation alone, we would then take actions $A_{{\sf t}}(F(X', {\sf t}))$, for which we would receive reward $R_{{\sf t}}(X, {\sf t}+1)$.
