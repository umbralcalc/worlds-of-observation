\chapter{\sffamily Optimising actions for control objectives}

{\bfseries\sffamily Concept.} The idea here is 

\section{\sffamily States, actions and attributing rewards}

\textcolor{red}{Rewrite the beginning of this section to talk about:
\begin{itemize}
\item{remove the need to parametric actions from this section --- actions are encoded as their own separate state variables in the history and the policy can be defined through $z$ which can change through a data update}
\item{using the online learning of state and parameters MAP}
\item{forward-predicting with this ensemble given an ensemble of policies to try, applying a discount to the forecasts to compute cumulative reward and then choosing the best route}
\item{once this pattern is working, the methodology can be adapted to improve the policy/action-generating function}    
\end{itemize}
}

In order to assess the quality of an agent's actions, we might later attribute a reward value ${\cal R}_{{\sf t}}$ for actions that were taken at timestep ${\sf t}$. Using a series of these rewards, a return value $R$ can also be computed using a future discount factor $\gamma$ like so 
%%
\begin{align}
R &= \sum_{{\sf t}=0}^\infty \gamma^{\sf t}{\cal R}_{\sf t} \,.
\end{align}
%%
A state-value function $V_\pi$ is defined as the expectation (under policy $\pi$) of return $R$, given state vector ${\cal S}_{\sf t}=s$, i.e.,
%%
\begin{align}
V_\pi (s) = {\rm E}_\pi (R\vert s) \,.
\end{align}
%%
Similarly, an action-value function $Q_\pi$ is defined as the expectation (again, under policy $\pi$) of return $R$, given state vector ${\cal S}_{\sf t}=s$ and action vector ${\cal A}_{\sf t}=a$, i.e.,
%%
\begin{align}
Q_\pi (s,a) = {\rm E}_\pi (R\vert s,a) \,.
\end{align}
%%

Follow-up this bit with the model-based approach that we're going to take in this book.
\begin{itemize}
\item{Talk through value and policy learning - in this book we will be doing the value learning with our generalised stochastic model and then the policy learning bit is more nuanced.}
\item{The value learning can be facilitated in software using a predictive model which is able to roll forecast rewards forward in time in a Monte Carlo fashion up to a window from a certain point given an input prior distribution of policies.}
\item{This input prior distribution of policies can itself be optimised by maximising expected utility in a Bayesian design framework!}
\end{itemize}