\chapter{\sffamily Optimising actions for control objectives}

{\bfseries\sffamily Concept.} The idea here is 

\section{\sffamily States, actions and attributing rewards}

\textcolor{red}{Rewrite the beginning of this section to talk about:
\begin{itemize}
\item{remove the need to parametric actions from this section --- actions are encoded as their own separate state variables in the history and the policy can be defined through $z$ which can change through a data update}
\item{using the online learning of state and parameters MAP}
\item{forward-predicting with this ensemble given an ensemble of policies to try, applying a discount to the forecasts to compute cumulative reward and then choosing the best route}
\item{once this pattern is working, the methodology can be adapted to improve the policy/action-generating function}    
\end{itemize}
%%
\begin{align}
P_{{\sf t}'{\sf t}}(A\vert Y,\theta ) &= \int_{\Omega_{{\sf t}}} {\rm d}X \, \Pi_{{\sf t}'{\sf t}}(A\vert X,\theta){\cal P}_{{\sf t}}(X\vert Y) \\
P_{{\sf t}'{\sf t}}(R\vert Y,\theta ) &= \int_{\Omega_{{\sf t}'}} {\rm d}X \int_{\aleph_{{\sf t}'}} {\rm d}A  \, P_{{\sf t}'}(R\vert X, A){\cal P}_{{\sf t}'{\sf t}}(X\vert Y)P_{{{\sf t}}'{\sf t}}(A\vert Y,\theta) \\
{\cal N}_{{\sf t}} &= \sum_{{\sf t}'={\sf t}}^{\infty}\left\{ \prod_{{\sf t}''={\sf t}'}^{{\sf t}} \gamma [\delta t({\sf t}')] \right\} \\
{\cal R}_{{\sf t}}(Y,\theta ) &= \frac{1}{{\cal N}}\sum_{{\sf t}'={\sf t}}^{\infty}\int_{\upsilon_{{\sf t}'}} {\rm d}r \,\left\{ \prod_{{\sf t}''={\sf t}'}^{{\sf t}} \gamma [\delta t({\sf t}')] \right\} r \,P_{{\sf t}'{\sf t}}(r\vert Y,\theta ) \\
\theta^*_{{\sf t}} &= {\rm argmax} \big[ {\cal R}_{{\sf t}}(Y,\theta )\big]
\end{align}
%%
}

In order to assess the quality of an agent's actions, we might later attribute a reward value ${\cal R}_{{\sf t}}$ for actions that were taken at timestep ${\sf t}$. Using a series of these rewards, a return value $R$ can also be computed using a future discount factor $\gamma$ like so 
%%
\begin{align}
{\cal R}_{{\sf t}} &= \sum_{{\sf t}'={\sf t}}^\infty \gamma^{{\sf t}'-{\sf t}}R_{{\sf t}'} \,.
\end{align}
%%
A state-value function $V_\pi$ is defined as the expectation (under policy $\pi$) of return ${\cal R}$, given state vector ${\cal S}_{\sf t}=s$, i.e.,
%%
\begin{align}
V_\pi (s) = {\rm E}_\pi ({\cal R}\vert s) \,.
\end{align}
%%
Similarly, an action-value function $Q_\pi$ is defined as the expectation (again, under policy $\pi$) of return ${\cal R}$, given state vector ${\cal S}_{\sf t}=s$ and action vector ${\cal A}_{\sf t}=a$, i.e.,
%%
\begin{align}
Q_\pi (s,a) = {\rm E}_\pi ({\cal R}\vert s,a) \,.
\end{align}
%%

Follow-up this bit with the model-based approach that we're going to take in this book.
\begin{itemize}
\item{Talk through value and policy learning - in this book we will be doing the value learning with our generalised stochastic model and then the policy learning bit is more nuanced.}
\item{The value learning can be facilitated in software using a predictive model which is able to roll forecast rewards forward in time in a Monte Carlo fashion up to a window from a certain point given an input prior distribution of policies.}
\item{This input prior distribution of policies can itself be optimised by maximising expected utility in a Bayesian design framework!}
\end{itemize}