\chapter{\sffamily Optimising actions for control objectives}

{\bfseries\sffamily Concept.} The idea here is 

\section{\sffamily States, actions and rewards}

\textcolor{red}{Need to edit this text below to match new mathematical formalism}

We observe the state at timestep ${\sf t}$ with ${\cal S}_{{\sf t}}(X_{{\sf t}}, z, {\sf t})$. In a Markov Decision Process (MDP), based on this observation alone, we would then take actions ${\cal A}_{{\sf t}}(F(X', z, {\sf t}))$, for which we would receive reward ${\cal R}_{{\sf t}}(X, z, {\sf t}+1)$.
