\chapter{\sffamily Optimising actions for control objectives}

{\bfseries\sffamily Concept.} The idea here is 

\section{\sffamily States, actions and formulating rewards}

We observe the state at timestep ${\sf t}$ with ${\cal S}_{{\sf t}} = M_{{\sf t}}(X', Z_{\sf t}, {\sf t})$. In a Markov Decision Process (MDP), based on this observation alone, we would then take actions $X_{{\sf t}+1} = {\cal F}_{{\sf t}+1}(X',Z_{\sf t},{\cal A}_{\sf t},{\sf t})$, for which we would later attribute reward ${\cal R}_{{\sf t}}$.
