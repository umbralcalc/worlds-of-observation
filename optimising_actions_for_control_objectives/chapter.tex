\chapter{\sffamily Optimising actions for control objectives}

{\bfseries\sffamily Concept.} The idea here is 

\section{\sffamily States, actions and attributing rewards}

\textcolor{red}{Rewrite the beginning of this section to talk about:
\begin{itemize}
\item{remove the need to parametric actions from this section --- actions are encoded as their own separate state variables in the history and the policy can be defined through $z$ which can change through a data update}
\item{using the online learning of state and parameters MAP}
\item{forward-predicting with this ensemble given an ensemble of policies to try, applying a discount to the forecasts to compute cumulative reward and then choosing the best route}
\item{once this pattern is working, the methodology can be adapted to improve the policy/action-generating function}    
\end{itemize}
%%
\begin{align}
P_{{\sf t}+1}(a\vert \theta^*_{{\sf t}+1}) &= \frac{1}{{\sf t}}\sum_{{\sf t}'=0}^{{\sf t}}\int_{\omega_{{\sf t}'}} {\rm d}x'P_{({\sf t}+1){\sf t}'}(a\vert x', A_{{\sf t}'},\theta^*_{{\sf t}+1}){\cal P}_{{\sf t}'}(x'\vert Y_{{\sf t}'}, Y_{{\sf t}'-1}, \dots) \\
{\cal R}_{{\sf t}+1}(\theta) &=(1-\gamma)\sum_{{\sf t}'={\sf t}}^{\infty}\int_{\upsilon_{{\sf t}'}} {\rm d}r' \gamma^{{\sf t}'-{\sf t}}r'P_{{\sf t}'}(r'\vert x', A_{{\sf t}'},\theta ){\cal P}_{{\sf t}'}(x'\vert Y') \\
\theta^*_{{\sf t}+1} &= {\rm argmax} \big[ {\cal R}_{{\sf t}+1}(\theta)\big]
\end{align}
%%
}

In order to assess the quality of an agent's actions, we might later attribute a reward value ${\cal R}_{{\sf t}}$ for actions that were taken at timestep ${\sf t}$. Using a series of these rewards, a return value $R$ can also be computed using a future discount factor $\gamma$ like so 
%%
\begin{align}
{\cal R}_{{\sf t}} &= \sum_{{\sf t}'={\sf t}}^\infty \gamma^{{\sf t}'-{\sf t}}R_{{\sf t}'} \,.
\end{align}
%%
A state-value function $V_\pi$ is defined as the expectation (under policy $\pi$) of return ${\cal R}$, given state vector ${\cal S}_{\sf t}=s$, i.e.,
%%
\begin{align}
V_\pi (s) = {\rm E}_\pi ({\cal R}\vert s) \,.
\end{align}
%%
Similarly, an action-value function $Q_\pi$ is defined as the expectation (again, under policy $\pi$) of return ${\cal R}$, given state vector ${\cal S}_{\sf t}=s$ and action vector ${\cal A}_{\sf t}=a$, i.e.,
%%
\begin{align}
Q_\pi (s,a) = {\rm E}_\pi ({\cal R}\vert s,a) \,.
\end{align}
%%

Follow-up this bit with the model-based approach that we're going to take in this book.
\begin{itemize}
\item{Talk through value and policy learning - in this book we will be doing the value learning with our generalised stochastic model and then the policy learning bit is more nuanced.}
\item{The value learning can be facilitated in software using a predictive model which is able to roll forecast rewards forward in time in a Monte Carlo fashion up to a window from a certain point given an input prior distribution of policies.}
\item{This input prior distribution of policies can itself be optimised by maximising expected utility in a Bayesian design framework!}
\end{itemize}