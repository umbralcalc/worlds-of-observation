\chapter{\sffamily Optimising actions for control objectives}

{\bfseries\sffamily Concept.} The idea here is 

\section{\sffamily States, actions and attributing rewards}    

Using the state value function $V$...
%%
\begin{align}
P_{({\sf t}+1){\sf t}}(r,X'\vert X, z,\theta) &= P_{{\sf t}+1}(r\vert X',A)\Pi_{({\sf t}+1){\sf t}}(A\vert X,\theta)P_{({\sf t}+1){\sf t}}(X'\vert X,z,A)  \\
{\cal N}_{{\sf t}} &= \sum_{{\sf t}'={\sf t}+1}^{\infty}\left\{ \prod_{{\sf t}''={\sf t}}^{{\sf t}'} \gamma [\delta t({\sf t}'')] \right\} \\
V_{{\sf t}}(X,z,\theta) &= \frac{1}{{\cal N}_{{\sf t}}}\sum_{{\sf t}'={\sf t}+1}^{\infty} \int_{\Omega_{{\sf t}'}}{\rm d}X'\int_{\rho_{{\sf t}'}} {\rm d}r \,r\, \prod_{{\sf t}''={\sf t}}^{{\sf t}'} \gamma [\delta t({\sf t}'')]P_{({\sf t}''+1){\sf t}''}(r,X'\vert X, z,\theta) \\
V_{{\sf t}}(X,z,\theta) &= \int_{\Omega_{{\sf t}+1}}{\rm d}X'\int_{\rho_{{\sf t}+1}} {\rm d}r \, P_{({\sf t}+1){\sf t}}(r,X'\vert X, z,\theta)\bigg\{ r+\gamma [\delta t({\sf t})]V_{{\sf t}+1}(X',z,\theta)\bigg\} \,.
\end{align}
%%
In either case, the optimal $\theta$, $\Pi$ and $V$ can be derived from
\begin{align}
\theta^*_{{\sf t}}(X,z) &= \underset{\theta}{{\rm argmax}} \big[ V_{{\sf t}}(X,z,\theta)\big] \\
\Pi^*_{({\sf t}+1){\sf t}}(A\vert X,z) &= \Pi_{({\sf t}+1){\sf t}}[A\vert X,\theta^*_{{\sf t}}(X,z)] \\
V^*_{{\sf t}}(X,z) &= V_{{\sf t}}[X,z,\theta^*_{{\sf t}}(X,z)] \,.
\end{align}

Follow-up this bit with the model-based approach that we're going to take in this book.
\begin{itemize}
\item{Looking at a stochastic policy iteration algorithm here combined with Monte Carlo rollouts.}
\item{The value learning can be facilitated in software using a predictive model which is able to roll forecast rewards forward in time in a Monte Carlo fashion up to a window from a certain point given an input prior distribution of policies.}
\item{This input prior distribution of policies can itself be optimised by maximising expected discounted utility in a Bayesian design framework. Draw parallels.}
\end{itemize}